{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWKGnw6dmqLG"
      },
      "source": [
        "# Scan2LOD Predictor\n",
        "### Prediction Logic that Classifies Facade Data [Images and Conflictmaps (3D-Data)] of the same building with the help of Convolutional Neural Networks.\n",
        "\n",
        "------------------------------------------------------------------------------\n",
        "## Input: 572x572 image (.jpg) and 572x572 conflict map (.png)\n",
        "------------------------------------------------------------------------------\n",
        "## Output: Class for each individual pixel\n",
        "\n",
        "The classes that are being output are:\n",
        "\n",
        "-Facade\n",
        "\n",
        "-Window\n",
        "\n",
        "-Door\n",
        "\n",
        "-Unknown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6n67QtRsc6O"
      },
      "source": [
        "# 1. IMPORTING LIBRARIES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftSvkOJTofOY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.utils import save_image\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm  # For progress bars\n",
        "import time\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An0cNdA3ktXn",
        "outputId": "11f5cd7c-d246-4e49-c338-79a593079769"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3PWRDJpsZzu"
      },
      "source": [
        "# 2. SETTING UP DIRECTORIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF0hXzMio3nU",
        "outputId": "2d3a4014-d226-450a-bc31-0b6e36480c40"
      },
      "outputs": [],
      "source": [
        "# base directory\n",
        "BASE_DIR = '/content/drive/MyDrive/Colab Notebooks/Scan2LOD_DATA'\n",
        "\n",
        "# Folders\n",
        "INPUT_DIR = os.path.join(BASE_DIR, 'Inputs')\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'Models')\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'Outputs')\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Define paths for specific types of inputs\n",
        "CONFLICT_MAP_DIR = os.path.join(INPUT_DIR, 'ConflictMaps')\n",
        "IMAGE_DIR = os.path.join(INPUT_DIR, 'FacadeImages')\n",
        "GROUND_TRUTH_DIR = os.path.join(INPUT_DIR, 'GroundTruth')  # If available\n",
        "\n",
        "# Define paths for model files\n",
        "UNET_MODEL_PATH = os.path.join(MODEL_DIR, 'unet_model.pth')\n",
        "MASKRCNN_MODEL_PATH = os.path.join(MODEL_DIR, 'maskrcnn_model.pth')\n",
        "\n",
        "# Create output subdirectories\n",
        "UNET_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'UNet')\n",
        "MASKRCNN_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'MaskRCNN')\n",
        "FUSION_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'Fusion')\n",
        "\n",
        "os.makedirs(UNET_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MASKRCNN_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FUSION_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Directory structure initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E44Y_vqNjRas"
      },
      "outputs": [],
      "source": [
        "# Adjust to change\n",
        "# FacadeImage and conflict map paths\n",
        "\n",
        "CONF_MAP_PATH = os.path.join(INPUT_DIR, 'ConflictMaps', 'Building57_ConflictMap.png')\n",
        "predictionPath = os.path.join(IMAGE_DIR, 'Building57_FacadeImage.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBmz-ircsyWt"
      },
      "source": [
        "# 3. MODEL ARCHITECTURES (U-Net and MaskRCNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyNXTzZHsUD3"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding='same')  # Using 'same' padding\n",
        "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
        "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
        "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding='same')\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
        "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding='same')\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding='same')\n",
        "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding='same')\n",
        "\n",
        "        # Decoder with size-matching upconvolution\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding='same')\n",
        "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding='same')\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding='same')\n",
        "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding='same')\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding='same')\n",
        "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding='same')\n",
        "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
        "\n",
        "        # Output layer\n",
        "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        xe11 = F.relu(self.e11(x))\n",
        "        xe12 = F.relu(self.e12(xe11))\n",
        "        xp1 = self.pool1(xe12)\n",
        "\n",
        "        xe21 = F.relu(self.e21(xp1))\n",
        "        xe22 = F.relu(self.e22(xe21))\n",
        "        xp2 = self.pool2(xe22)\n",
        "\n",
        "        xe31 = F.relu(self.e31(xp2))\n",
        "        xe32 = F.relu(self.e32(xe31))\n",
        "        xp3 = self.pool3(xe32)\n",
        "\n",
        "        xe41 = F.relu(self.e41(xp3))\n",
        "        xe42 = F.relu(self.e42(xe41))\n",
        "        xp4 = self.pool4(xe42)\n",
        "\n",
        "        xe51 = F.relu(self.e51(xp4))\n",
        "        xe52 = F.relu(self.e52(xe51))\n",
        "\n",
        "        # Decoder with size checking\n",
        "        xu1 = self.upconv1(xe52)\n",
        "        if xu1.size() != xe42.size():\n",
        "            xu1 = F.interpolate(xu1, size=xe42.size()[2:])\n",
        "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
        "        xd11 = F.relu(self.d11(xu11))\n",
        "        xd12 = F.relu(self.d12(xd11))\n",
        "\n",
        "        xu2 = self.upconv2(xd12)\n",
        "        if xu2.size() != xe32.size():\n",
        "            xu2 = F.interpolate(xu2, size=xe32.size()[2:])\n",
        "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
        "        xd21 = F.relu(self.d21(xu22))\n",
        "        xd22 = F.relu(self.d22(xd21))\n",
        "\n",
        "        xu3 = self.upconv3(xd22)\n",
        "        if xu3.size() != xe22.size():\n",
        "            xu3 = F.interpolate(xu3, size=xe22.size()[2:])\n",
        "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
        "        xd31 = F.relu(self.d31(xu33))\n",
        "        xd32 = F.relu(self.d32(xd31))\n",
        "\n",
        "        xu4 = self.upconv4(xd32)\n",
        "        if xu4.size() != xe12.size():\n",
        "            xu4 = F.interpolate(xu4, size=xe12.size()[2:])\n",
        "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
        "        xd41 = F.relu(self.d41(xu44))\n",
        "        xd42 = F.relu(self.d42(xd41))\n",
        "\n",
        "        # Output layer\n",
        "        out = self.outconv(xd42)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNOtgXT6Saoc"
      },
      "outputs": [],
      "source": [
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy86T4_RtEji"
      },
      "source": [
        "# 4. LOADING MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK_C153dNGbJ"
      },
      "outputs": [],
      "source": [
        "def load_unet_model(model_path, num_classes=4, device='cuda'):\n",
        "    \"\"\"Load a trained U-Net model\"\"\"\n",
        "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "    model = UNet(n_class=num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(f\"U-Net model loaded from: {model_path}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khtyULqAtF_i"
      },
      "outputs": [],
      "source": [
        "def load_maskrcnn_model(model_path, device='cuda'):\n",
        "    \"\"\"\n",
        "    Load a trained Mask R-CNN model\n",
        "    \"\"\"\n",
        "    # This is a placeholder - adjust based on your Mask R-CNN architecture\n",
        "    try:\n",
        "\n",
        "        # Initialize model with pre-trained weights\n",
        "        num_classes = 2\n",
        "        device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "        model = get_instance_segmentation_model(num_classes)\n",
        "        weights = torch.load(model_path)\n",
        "\n",
        "        # Load your trained weights\n",
        "        model.load_state_dict(weights)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        print(f\"Mask R-CNN model loaded from: {model_path}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Mask R-CNN model: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_fEpazkOy1O"
      },
      "outputs": [],
      "source": [
        "def get_probabilities(self, x):\n",
        "    \"\"\"Get both raw logits and normalized probabilities\"\"\"\n",
        "    logits = self.forward(x)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return logits, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ImnFqR8OhPC"
      },
      "outputs": [],
      "source": [
        "def preprocess_conflict_map(conflict_map_path, device):\n",
        "    \"\"\"Preprocess a conflict map for the U-Net model\"\"\"\n",
        "    # Color mappings for conflict map\n",
        "    CONFLICT_MAP_COLORS = {\n",
        "        'confirming': (0, 255, 0),  # Green - Confirming\n",
        "        'unknown': (0, 0, 255),     # Blue - Unknown\n",
        "        'conflict': (255, 0, 0)     # Red - Conflict\n",
        "    }\n",
        "\n",
        "    # Load conflict map as RGB\n",
        "    conf_map = Image.open(conflict_map_path).convert('RGB')\n",
        "    conf_map_array = np.array(conf_map)\n",
        "\n",
        "    # Create input channels based on different conflict map states\n",
        "    confirming_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['confirming'], axis=2)\n",
        "    unknown_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['unknown'], axis=2)\n",
        "    conflict_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['conflict'], axis=2)\n",
        "\n",
        "    # Stack the masks into a 3-channel tensor\n",
        "    conf_map_tensor = np.stack([confirming_mask, unknown_mask, conflict_mask], axis=0)\n",
        "    conf_map_tensor = torch.from_numpy(conf_map_tensor).float().unsqueeze(0).to(device)\n",
        "\n",
        "    return conf_map_tensor, conf_map_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OphrfVLgOlTE"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, device, target_size=(572, 572)):\n",
        "    # Load and normalize image for Mask R-CNN\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize(target_size, Image.LANCZOS)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    return image_tensor, np.array(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D73_0z53Razj"
      },
      "outputs": [],
      "source": [
        "def preprocess_conflict_map(conflict_map_path, device):\n",
        "    \"\"\"Preprocess a conflict map for the U-Net model\"\"\"\n",
        "    # Color mappings for conflict map\n",
        "    CONFLICT_MAP_COLORS = {\n",
        "        'confirming': (0, 255, 0),  # Green - Confirming\n",
        "        'unknown': (0, 0, 255),     # Blue - Unknown\n",
        "        'conflict': (255, 0, 0)     # Red - Conflict\n",
        "    }\n",
        "\n",
        "    # Load conflict map as RGB\n",
        "    conf_map = Image.open(conflict_map_path).convert('RGB')\n",
        "    conf_map_array = np.array(conf_map)\n",
        "\n",
        "    # Create input channels based on different conflict map states\n",
        "    confirming_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['confirming'], axis=2)\n",
        "    unknown_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['unknown'], axis=2)\n",
        "    conflict_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['conflict'], axis=2)\n",
        "\n",
        "    # Stack the masks into a 3-channel tensor\n",
        "    conf_map_tensor = np.stack([confirming_mask, unknown_mask, conflict_mask], axis=0)\n",
        "    conf_map_tensor = torch.from_numpy(conf_map_tensor).float().unsqueeze(0).to(device)\n",
        "\n",
        "    return conf_map_tensor, conf_map_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjX9h9FGR7ye"
      },
      "outputs": [],
      "source": [
        "def test_unet_output(model, conflict_map_path, device='cuda'):\n",
        "    \"\"\"Test UNet model on a conflict map and visualize outputs\"\"\"\n",
        "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Preprocess conflict map\n",
        "    conf_map_tensor, conf_map_array = preprocess_conflict_map(conflict_map_path, device)\n",
        "\n",
        "    # Forward pass through UNet\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get raw logits from model\n",
        "        logits = model(conf_map_tensor)\n",
        "\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Get class predictions\n",
        "        pred_classes = torch.argmax(logits, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "        # Convert to numpy for visualization\n",
        "        logits_np = logits.squeeze().cpu().numpy()\n",
        "        probs_np = probs.squeeze().cpu().numpy()\n",
        "\n",
        "    # Define class names for visualization\n",
        "    class_names = ['Facade', 'Window', 'Door', 'Unknown']\n",
        "\n",
        "    # Visualize input\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(conf_map_array)\n",
        "    plt.title('Input Conflict Map')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize raw logits\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.imshow(logits_np[i], cmap='viridis')\n",
        "        plt.colorbar()\n",
        "        plt.title(f'Raw Logits: {class_name}')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize probabilities\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.imshow(probs_np[i], cmap='viridis', vmin=0, vmax=1)\n",
        "        plt.colorbar()\n",
        "        plt.title(f'Probability: {class_name}')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize predicted class\n",
        "    class_colors = {\n",
        "        0: [105, 105, 105],  # Facade - Gray\n",
        "        1: [255, 255, 0],    # Window - Yellow\n",
        "        2: [139, 69, 19],    # Door - Brown\n",
        "        3: [220, 220, 220]   # Unknown - Light Gray\n",
        "    }\n",
        "\n",
        "    # Convert prediction to RGB\n",
        "    rgb_pred = np.zeros((pred_classes.shape[0], pred_classes.shape[1], 3), dtype=np.uint8)\n",
        "    for class_idx, color in class_colors.items():\n",
        "        rgb_pred[pred_classes == class_idx] = color\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(rgb_pred)\n",
        "    plt.title('UNet Prediction')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics for each class\n",
        "    print(\"Class probability statistics:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"\\n{class_name}:\")\n",
        "        print(f\"  Min: {probs_np[i].min():.4f}\")\n",
        "        print(f\"  Max: {probs_np[i].max():.4f}\")\n",
        "        print(f\"  Mean: {probs_np[i].mean():.4f}\")\n",
        "        print(f\"  Pixels predicted as {class_name}: {np.sum(pred_classes == i)} ({np.sum(pred_classes == i)/pred_classes.size*100:.2f}%)\")\n",
        "\n",
        "    # Return processed data for further analysis\n",
        "    return {\n",
        "        'raw_logits': logits_np,\n",
        "        'probabilities': probs_np,\n",
        "        'predictions': pred_classes\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjADFmIaO-S-"
      },
      "source": [
        "## 5. Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnI-J2okPET8"
      },
      "outputs": [],
      "source": [
        "def get_unet_predictions(model, conflict_map_tensor):\n",
        "    with torch.no_grad():\n",
        "        logits, probabilities = model.get_probabilities(conflict_map_tensor)\n",
        "        predictions = torch.argmax(logits, dim=1).squeeze().cpu().numpy()\n",
        "        probabilities = probabilities.squeeze().cpu().numpy()\n",
        "\n",
        "    return predictions, probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "160EW4VQSDJF",
        "outputId": "e0c13467-b44b-4310-d6b2-31cb9f59dfae"
      },
      "outputs": [],
      "source": [
        "UNET_PATH = os.path.join(MODEL_DIR, 'Facade_model_b14_e50_CMPGENREAL_nS.pth')\n",
        "\n",
        "# Load model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "unet_model = load_unet_model(UNET_PATH, num_classes=4, device=device)\n",
        "\n",
        "# Test model output\n",
        "unet_results = test_unet_output(unet_model, CONF_MAP_PATH, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdcmOk8wVfmk",
        "outputId": "188b1d5e-3fef-4573-eacf-27184f104905"
      },
      "outputs": [],
      "source": [
        "device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "MaskRCNN_windows_model = load_maskrcnn_model(os.path.join(MODEL_DIR, 'MASK_RCNN_onlyWindows.pth'))\n",
        "MaskRCNN_doors_model = load_maskrcnn_model(os.path.join(MODEL_DIR, 'MASK_RCNN_doors.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrIl6peohnSM"
      },
      "outputs": [],
      "source": [
        "predImg = Image.open(predictionPath).convert(\"RGB\")\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "img_tensor = transform(predImg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "collapsed": true,
        "id": "FbmnSryFhO0U",
        "outputId": "717a194d-8ed5-4036-95a6-e21dacef2ca5"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(img_tensor.mul(255).permute(1, 2, 0).byte().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzVz-ZKq9KwJ"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    prediction = MaskRCNN_windows_model([img_tensor.to(device)])\n",
        "selected_photo = prediction[0]\n",
        "selected_masks = selected_photo['masks']\n",
        "\n",
        "#squeeze extra dimension given to the masks\n",
        "selected_masks = torch.squeeze(selected_masks, 1)\n",
        "\n",
        "#test = selected_masks[1] #first element of the tensor = first instance mask\n",
        "num_of_masks = selected_masks.size(dim=0)\n",
        "x_of_masks = selected_masks.size(dim=1)\n",
        "y_of_masks = selected_masks.size(dim=2)\n",
        "\n",
        "list_of_arrays = []\n",
        "final_mask_windows = selected_masks.sum(axis=0)\n",
        "for i in range(num_of_masks):\n",
        "  np_arr = selected_masks[i].cpu().detach().numpy()\n",
        "  list_of_arrays.append(np_arr)\n",
        "\n",
        "one_composite_mask_windows = sum(list_of_arrays)\n",
        "\n",
        "# save_path = path + \"/results/wallBmaskFull.jpg\"\n",
        "# save_image(selected_masks, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2p3ADXfi4qD"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    prediction = MaskRCNN_doors_model([img_tensor.to(device)])\n",
        "selected_photo = prediction[0]\n",
        "selected_masks = selected_photo['masks']\n",
        "\n",
        "#squeeze extra dimension given to the masks\n",
        "selected_masks = torch.squeeze(selected_masks, 1)\n",
        "\n",
        "#test = selected_masks[1] #first element of the tensor = first instance mask\n",
        "num_of_masks = selected_masks.size(dim=0)\n",
        "x_of_masks = selected_masks.size(dim=1)\n",
        "y_of_masks = selected_masks.size(dim=2)\n",
        "\n",
        "list_of_arrays = []\n",
        "final_mask_doors = selected_masks.sum(axis=0)\n",
        "for i in range(num_of_masks):\n",
        "  np_arr = selected_masks[i].cpu().detach().numpy()\n",
        "  list_of_arrays.append(np_arr)\n",
        "\n",
        "one_composite_mask_doors = sum(list_of_arrays)\n",
        "\n",
        "# save_path = path + \"/results/wallBmaskFull.jpg\"\n",
        "# save_image(selected_masks, save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d9m-hHfRhsu-",
        "outputId": "53751fc1-db91-40b9-8983-743f2fea9fa6"
      },
      "outputs": [],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "BIi2nNb8jNZ0",
        "outputId": "9b3cb3f1-ec20-4af8-a650-684892d51742"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(final_mask_windows.mul(255).byte().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "riXf6uqI-Dk9",
        "outputId": "c9483e2a-ad53-4faa-8733-0898c92812dd"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(final_mask_doors.mul(255).byte().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP57gVjDwugd"
      },
      "outputs": [],
      "source": [
        "def normalize_maskrcnn_output(mask, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Normalize Mask R-CNN output to scale pixel values between 0 and 1.\n",
        "    Simply finds global min and max and scales all values.\n",
        "\n",
        "    Parameters:\n",
        "    mask: Instance masks from Mask R-CNN\n",
        "    epsilon: Small value to prevent division by zero\n",
        "\n",
        "    Returns:\n",
        "    Normalized probability tensor\n",
        "    \"\"\"\n",
        "    # Move tensor to CPU and convert to numpy\n",
        "    logits = mask.cpu().numpy()\n",
        "\n",
        "    # Get the global min and max across the entire array, regardless of shape\n",
        "    min_val = np.min(logits)\n",
        "    max_val = np.max(logits)\n",
        "\n",
        "    # Simple min-max normalization\n",
        "    range_val = max_val - min_val\n",
        "    if range_val < epsilon:\n",
        "        # If there's no variation, return zeros in the original shape\n",
        "        return np.zeros_like(logits)\n",
        "\n",
        "    # Normalize to [0,1] range\n",
        "    normalized = (logits - min_val) / range_val\n",
        "\n",
        "    # If we need a single 2D image as output, take the max across instance dimension if it exists\n",
        "    if len(normalized.shape) == 3:\n",
        "        normalized = np.max(normalized, axis=0)\n",
        "\n",
        "    return normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAB4BvK49imD"
      },
      "outputs": [],
      "source": [
        "## Unet output\n",
        "unet_probs = unet_results['probabilities']  # Shape [4, height, width]\n",
        "#unet_pred = unet_results['predictions']    # Shape [height, width]\n",
        "\n",
        "## MaskRCNN outputs\n",
        "window_probs = normalize_maskrcnn_output(final_mask_windows)\n",
        "window_probs.reshape((572, 572))\n",
        "\n",
        "#windows_blinds_probs = normalize_maskrcnn_output(final_mask_windows_blinds)\n",
        "#windows_blinds_probs.reshape((572, 572))\n",
        "\n",
        "#door_probs = window_probs + windows_blinds_probs\n",
        "\n",
        "\n",
        "\n",
        "# Apply the condition using NumPy's boolean indexing\n",
        "#door_probs[door_probs < 1] = 0  # Set values above X to zero\n",
        "\n",
        "door_probs = normalize_maskrcnn_output(final_mask_doors)\n",
        "\n",
        "# Renormalize after addition\n",
        "if np.max(door_probs) > 0:\n",
        "    door_probs = door_probs / np.max(door_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "3Olg5iDR7gFM",
        "outputId": "8910f0a3-01e3-4ef5-b459-c4d0bf527626"
      },
      "outputs": [],
      "source": [
        "# Plot the heatmap\n",
        "plt.figure(figsize=(6, 6))\n",
        "#plt.imshow(unet_probs[1], cmap=\"viridis\", interpolation=\"nearest\")\n",
        "#plt.imshow(window_probs, cmap=\"viridis\", interpolation=\"nearest\")\n",
        "\n",
        "\n",
        "\n",
        "plt.imshow(final_mask_windows.mul(255).byte().cpu().numpy()/255, cmap=\"viridis\", interpolation=\"nearest\")\n",
        "\n",
        "plt.colorbar(label=\"Probability Value\")\n",
        "plt.title(\"Visualization of window_probs Array\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1XVMBK60B51"
      },
      "source": [
        "## 6. Semantic Fusion\n",
        "\n",
        "The fusion of the predictions happens with a set of hyperparameters that form our final estimate by weighting the estimates of Windows and Doors segmented by our Unet with the estimates from the MaskRCNN. Facade and Unknown classes will be directly taken from the UNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTVWbC9eFEdg"
      },
      "outputs": [],
      "source": [
        "def combine_predictions_linear(unet_probs, window_probs, door_probs):\n",
        "    \"\"\"\n",
        "    Create a linear weighted combination of UNet and Mask R-CNN predictions\n",
        "\n",
        "    Parameters:\n",
        "        unet_probs: UNet probabilities [4, height, width] - all classes\n",
        "        window_probs: Window probabilities from Mask R-CNN [height, width]\n",
        "        door_probs: Door probabilities from Mask R-CNN [height, width], optional\n",
        "        window_weight: Weight for Mask R-CNN window prediction (1-window_weight for UNet)\n",
        "        door_weight: Weight for Mask R-CNN door prediction (1-door_weight for UNet)\n",
        "\n",
        "    Returns:\n",
        "        combined_probs: Combined probability tensor [4, height, width]\n",
        "    \"\"\"\n",
        "    # Create a copy of UNet probabilities to avoid modifying original\n",
        "    combined_probs = unet_probs.copy()\n",
        "\n",
        "    # For window class [1], apply weighted combination\n",
        "    if window_probs is not None:\n",
        "        window_alpha = 3\n",
        "        window_beta = 3 #3 was valid for 57 // 6 was valid for 57_2\n",
        "        combined_probs[1] = window_alpha * unet_probs[1] + window_beta * window_probs\n",
        "\n",
        "    # For door class [2], apply weighted combination if available\n",
        "    if door_probs is not None:\n",
        "        door_alpha = 4.5\n",
        "        door_beta = 1\n",
        "        combined_probs[2] = door_alpha * unet_probs[2] + door_beta * door_probs\n",
        "\n",
        "    # Unknowns\n",
        "    combined_probs[3] = 6* combined_probs[3]\n",
        "\n",
        "    # Normalize probabilities to sum to 1 for each pixel\n",
        "    sum_probs = np.sum(combined_probs, axis=0)\n",
        "    for i in range(combined_probs.shape[0]):\n",
        "        combined_probs[i] /= sum_probs\n",
        "\n",
        "    return combined_probs\n",
        "\n",
        "def adaptive_combine_predictions(unet_probs, window_probs, door_probs=None,\n",
        "                               window_threshold=0.3, door_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Create an adaptive weighted combination based on UNet confidence\n",
        "\n",
        "    Parameters:\n",
        "        unet_probs: UNet probabilities [4, height, width] - all classes\n",
        "        window_probs: Window probabilities from Mask R-CNN [height, width]\n",
        "        door_probs: Door probabilities from Mask R-CNN [height, width], optional\n",
        "        window_threshold: Confidence threshold for windows\n",
        "        door_threshold: Confidence threshold for doors\n",
        "\n",
        "    Returns:\n",
        "        combined_probs: Combined probability tensor [4, height, width]\n",
        "    \"\"\"\n",
        "    # Create a copy of UNet probabilities to avoid modifying original\n",
        "    combined_probs = unet_probs.copy()\n",
        "\n",
        "\n",
        "    # Calculate UNet confidence for each class\n",
        "    window_confidence = unet_probs[1]\n",
        "    door_confidence = unet_probs[2]\n",
        "\n",
        "    # WINDOWS: use Mask R-CNN more where UNet is less confident\n",
        "    if window_probs is not None:\n",
        "        # Calculate adaptive weights based on confidence\n",
        "        window_weight = np.where(window_confidence < window_threshold, 0.8, 0.2)\n",
        "\n",
        "        # Apply weighted combination\n",
        "        combined_probs[1] = (1 - window_weight) * unet_probs[1] + window_weight * window_probs\n",
        "\n",
        "    # DOORS: use Mask R-CNN more where UNet is less confident\n",
        "    if door_probs is not None:\n",
        "        # Calculate adaptive weights based on confidence\n",
        "        door_weight = np.where(door_confidence < door_threshold, 0.8, 0.2)\n",
        "\n",
        "        # Apply weighted combination\n",
        "        combined_probs[2] = (1 - door_weight) * unet_probs[2] + door_weight * door_probs\n",
        "\n",
        "    # Normalize probabilities to sum to 1 for each pixel\n",
        "    sum_probs = np.sum(combined_probs, axis=0)\n",
        "    for i in range(combined_probs.shape[0]):\n",
        "        combined_probs[i] /= sum_probs\n",
        "\n",
        "    return combined_probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-tEiONmHRDG"
      },
      "outputs": [],
      "source": [
        "combined_probs = combine_predictions_linear(\n",
        "     unet_probs,            # From UNet\n",
        "     window_probs,          # From Window Mask R-CNN\n",
        "     door_probs\n",
        " )\n",
        "\n",
        "#combined_probs = adaptive_combine_predictions(\n",
        " #    unet_probs,            # From UNet\n",
        " #    window_probs,          # From Window Mask R-CNN\n",
        " #    door_probs,            # From Door Mask R-CNN (optional)\n",
        " #    window_threshold=0.4,\n",
        " #    door_threshold=0.4\n",
        " #)\n",
        "\n",
        " # Get final prediction\n",
        "final_pred = np.argmax(combined_probs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIdOfe9DH05a"
      },
      "source": [
        "## 7. Final Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFi7J9ROHsPL"
      },
      "outputs": [],
      "source": [
        "def visualize_final_prediction(pred, original_image=None, save_path=None, show_plot=True):\n",
        "    \"\"\"\n",
        "    Visualize the final prediction with optional comparison to inputs and individual model outputs\n",
        "\n",
        "    Parameters:\n",
        "        pred: Final prediction array of shape [height, width] with class indices\n",
        "        original_image: Original input image (optional)\n",
        "        unet_pred: UNet prediction array (optional)\n",
        "        maskrcnn_pred: Mask R-CNN prediction array (optional)\n",
        "        save_path: Path to save the visualization (optional)\n",
        "        show_plot: Whether to display the plot (default True)\n",
        "    \"\"\"\n",
        "    # Define class colors\n",
        "    class_colors = {\n",
        "        0: [105, 105, 105],  # Facade - Gray\n",
        "        1: [255, 255, 0],    # Window - Yellow\n",
        "        2: [139, 69, 19],    # Door - Brown\n",
        "        3: [220, 220, 220]   # Unknown - Light Gray\n",
        "    }\n",
        "\n",
        "    # Function to convert prediction to RGB\n",
        "    def pred_to_rgb(pred_array):\n",
        "        height, width = pred_array.shape\n",
        "        rgb = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "        for class_idx, color in class_colors.items():\n",
        "            mask = (pred_array == class_idx)\n",
        "            rgb[mask] = color\n",
        "        return rgb\n",
        "\n",
        "    # Convert final prediction to RGB\n",
        "    rgb_pred = pred_to_rgb(pred)\n",
        "\n",
        "    # Determine the number of subplots needed\n",
        "    num_plots = 1\n",
        "    if original_image is not None:\n",
        "        num_plots += 1\n",
        "\n",
        "    # Create figure with appropriate size\n",
        "    fig = plt.figure(figsize=(5*num_plots, 5))\n",
        "\n",
        "    # Plot counter\n",
        "    plot_idx = 1\n",
        "\n",
        "    # Original image\n",
        "    if original_image is not None:\n",
        "        plt.subplot(1, num_plots, plot_idx)\n",
        "        plt.imshow(original_image)\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis('off')\n",
        "        plot_idx += 1\n",
        "\n",
        "    # Final prediction\n",
        "    plt.subplot(1, num_plots, plot_idx)\n",
        "    plt.imshow(rgb_pred)\n",
        "    plt.title(\"Final Prediction\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Add a legend\n",
        "    legend_elements = []\n",
        "    class_names = [\"Facade\", \"Window\", \"Door\", \"Unknown\"]\n",
        "    for i, (class_idx, color) in enumerate(class_colors.items()):\n",
        "        # Convert RGB to matplotlib format (0-1)\n",
        "        mpl_color = [c/255 for c in color]\n",
        "        legend_elements.append(plt.Rectangle((0,0), 1, 1, color=mpl_color, label=class_names[i]))\n",
        "\n",
        "    # Add the legend to the right side of the plot\n",
        "    fig.legend(handles=legend_elements, loc='center right', title=\"Classes\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout for legend\n",
        "\n",
        "    # Save if requested\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Visualization saved to {save_path}\")\n",
        "\n",
        "    # Show if requested\n",
        "    if show_plot:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "    return rgb_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "i90MSvIdIC9T",
        "outputId": "b7f40c7e-31c4-4e58-a64e-3b527355f5bf"
      },
      "outputs": [],
      "source": [
        "# Get the final prediction\n",
        "final_pred = np.argmax(combined_probs, axis=0)\n",
        "\n",
        "# Visualize with comparisons\n",
        "original_image = np.array(Image.open(predictionPath).convert(\"RGB\"))\n",
        "visualize_final_prediction(\n",
        "    final_pred,\n",
        "    original_image=original_image,\n",
        "    save_path=os.path.join(OUTPUT_DIR, \"fusion_result.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3offsn6A0pdw"
      },
      "source": [
        "#3D Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ojbJoq6UHR"
      },
      "source": [
        "# 8. Reconstruction with WallMatching\n",
        "\n",
        "- export unter LOD3 output\n",
        "- brauchen die Facade ID von der Frontfacade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aqVVDoYb-_L",
        "outputId": "66112b29-0e9e-4980-fef2-bce7d37cadd5"
      },
      "outputs": [],
      "source": [
        "# Check if the repository already exists\n",
        "import os\n",
        "\n",
        "if not os.path.exists('LoD3Framework-'):\n",
        "    # Clone the repository if it doesn't exist\n",
        "    !git clone https://github.com/wangyuefeng2017/LoD3Framework-.git\n",
        "    print(\"Repository cloned successfully!\")\n",
        "else:\n",
        "    print(\"Repository already exists, skipping clone.\")\n",
        "\n",
        "# Change to the repository directory\n",
        "%cd LoD3Framework-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4A0iC0KcCRU"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install -q joblib lxml opencv-python\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from lxml import etree\n",
        "\n",
        "# Import necessary modules from the framework\n",
        "sys.path.append('.')\n",
        "import readgml1\n",
        "import initpara_batch3_stage as initpara_batch3\n",
        "import amendwindow22_stage_PATCH as optimal\n",
        "import creat1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOcIpcaZcMzm"
      },
      "outputs": [],
      "source": [
        "def clean_predictions(pred, min_window_area=500, min_door_area=600):\n",
        "    \"\"\"\n",
        "    Clean predictions with better window separation.\n",
        "\n",
        "    Args:\n",
        "        pred: Prediction array with class indices (0=facade, 1=window, 2=door, 3=unknown)\n",
        "        min_window_area: Minimum area in pixels for windows to be kept\n",
        "        min_door_area: Minimum area in pixels for doors to be kept\n",
        "    \"\"\"\n",
        "    cleaned_pred = pred.copy()\n",
        "\n",
        "    # Process windows (class 1)\n",
        "    window_mask = (cleaned_pred == 1).astype(np.uint8)\n",
        "\n",
        "    # First apply erosion to separate potentially connected components\n",
        "    window_mask = cv2.erode(window_mask, np.ones((3, 3), np.uint8), iterations=2)\n",
        "\n",
        "    # Then dilate slightly to restore size without reconnecting\n",
        "    window_mask = cv2.dilate(window_mask, np.ones((2, 2), np.uint8), iterations=1)\n",
        "\n",
        "    # Remove small window components\n",
        "    num_components, window_labels = cv2.connectedComponents(window_mask)\n",
        "    for label in range(1, num_components):\n",
        "        component = (window_labels == label)\n",
        "        area = np.sum(component)\n",
        "        if area < min_window_area:\n",
        "            window_mask[component] = 0\n",
        "\n",
        "    # Process doors (class 2)\n",
        "    door_mask = (cleaned_pred == 2).astype(np.uint8)\n",
        "\n",
        "    # First apply erosion to separate potentially connected components\n",
        "    door_mask = cv2.erode(door_mask, np.ones((3, 3), np.uint8), iterations=2)\n",
        "\n",
        "    # Then dilate slightly to restore size without reconnecting\n",
        "    door_mask = cv2.dilate(door_mask, np.ones((2, 2), np.uint8), iterations=1)\n",
        "\n",
        "    # Remove small door components\n",
        "    num_components, door_labels = cv2.connectedComponents(door_mask)\n",
        "    for label in range(1, num_components):\n",
        "        component = (door_labels == label)\n",
        "        area = np.sum(component)\n",
        "        if area < min_door_area:\n",
        "            door_mask[component] = 0\n",
        "\n",
        "    # Enhance facade areas to ensure they separate windows/doors\n",
        "    facade_mask = np.ones_like(window_mask)\n",
        "    facade_mask[window_mask == 1] = 0\n",
        "    facade_mask[door_mask == 1] = 0\n",
        "\n",
        "    # Dilate facade to ensure separation\n",
        "    facade_mask = cv2.dilate(facade_mask, np.ones((3, 3), np.uint8), iterations=1)\n",
        "\n",
        "    # Apply the dilated facade to ensure separation\n",
        "    window_mask[facade_mask == 1] = 0\n",
        "    door_mask[facade_mask == 1] = 0\n",
        "\n",
        "    # Rebuild the prediction\n",
        "    cleaned_pred = np.zeros_like(pred)\n",
        "    cleaned_pred[window_mask == 1] = 1\n",
        "    cleaned_pred[door_mask == 1] = 2\n",
        "    cleaned_pred[(window_mask == 0) & (door_mask == 0) & (pred != 3)] = 0\n",
        "    cleaned_pred[pred == 3] = 3\n",
        "\n",
        "    return cleaned_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQDFsHltcP5L"
      },
      "outputs": [],
      "source": [
        "def scale_prediction_to_wall_surface(pred, wall_width, wall_height):\n",
        "    \"\"\"\n",
        "    Scale prediction to match wall surface dimensions while preserving aspect ratio.\n",
        "\n",
        "    Args:\n",
        "        pred: Prediction array (572x572 or other size)\n",
        "        wall_width: Actual width of the wall surface\n",
        "        wall_height: Actual height of the wall surface\n",
        "\n",
        "    Returns:\n",
        "        scaled_pred: Prediction scaled to match wall surface\n",
        "    \"\"\"\n",
        "    orig_height, orig_width = pred.shape\n",
        "\n",
        "    # Calculate target size preserving aspect ratio\n",
        "    wall_ratio = wall_width / wall_height\n",
        "    pred_ratio = orig_width / orig_height\n",
        "\n",
        "    if wall_ratio > pred_ratio:\n",
        "        # Wall is wider than prediction\n",
        "        target_width = int(orig_height * wall_ratio)\n",
        "        target_height = orig_height\n",
        "    else:\n",
        "        # Wall is taller than prediction\n",
        "        target_width = orig_width\n",
        "        target_height = int(orig_width / wall_ratio)\n",
        "\n",
        "    # Resize prediction using nearest neighbor to maintain class values\n",
        "    scaled_pred = cv2.resize(pred.astype(np.uint8), (target_width, target_height),\n",
        "                           interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    print(f\"Scaled prediction from {orig_width}x{orig_height} to {target_width}x{target_height}\")\n",
        "    print(f\"Original aspect ratio: {pred_ratio:.2f}, Wall aspect ratio: {wall_ratio:.2f}\")\n",
        "\n",
        "    return scaled_pred\n",
        "\n",
        "def convert_prediction_to_segment_data(pred, building_id, facade_id, segment_dir):\n",
        "    \"\"\"\n",
        "    Convert prediction to segment data expected by LoD3Framework.\n",
        "\n",
        "    Args:\n",
        "        pred: Prediction array\n",
        "        building_id: ID of the building\n",
        "        facade_id: ID of the facade\n",
        "        segment_dir: Directory to save segment data\n",
        "\n",
        "    Returns:\n",
        "        segment_file_path: Path to saved segment data\n",
        "        splash_path: Path to visualization\n",
        "        window_count: Number of windows detected\n",
        "        door_count: Number of doors detected\n",
        "    \"\"\"\n",
        "    # Create a unique identifier for this facade\n",
        "    safe_facade_id = facade_id.replace('/', '_').replace('\\\\', '_')\n",
        "    file_prefix = f\"{building_id}_{safe_facade_id}\"\n",
        "\n",
        "    # Find contours for windows (class 1)\n",
        "    window_mask = (pred == 1).astype(np.uint8)\n",
        "    window_contours, _ = cv2.findContours(window_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    window_boxes = []\n",
        "    for contour in window_contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area < 50:  # Final small contour filter\n",
        "            continue\n",
        "        confidence = 0.95\n",
        "        window_boxes.append({\n",
        "            \"x\": x, \"y\": y, \"width\": w, \"height\": h,\n",
        "            \"class\": \"window\", \"confidence\": confidence, \"area\": area\n",
        "        })\n",
        "\n",
        "    # Find contours for doors (class 2)\n",
        "    door_mask = (pred == 2).astype(np.uint8)\n",
        "    door_contours, _ = cv2.findContours(door_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    door_boxes = []\n",
        "    for contour in door_contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area < 100:  # Final small contour filter\n",
        "            continue\n",
        "        confidence = 0.95\n",
        "        door_boxes.append({\n",
        "            \"x\": x, \"y\": y, \"width\": w, \"height\": h,\n",
        "            \"class\": \"door\", \"confidence\": confidence, \"area\": area\n",
        "        })\n",
        "\n",
        "    # Sort by area (largest first) to prioritize major elements\n",
        "    window_boxes = sorted(window_boxes, key=lambda box: box[\"area\"], reverse=True)\n",
        "    door_boxes = sorted(door_boxes, key=lambda box: box[\"area\"], reverse=True)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Facade {facade_id}:\")\n",
        "    print(f\"  - Found {len(window_boxes)} windows and {len(door_boxes)} doors\")\n",
        "\n",
        "    # Only keep position and size info for LoD3Framework compatibility\n",
        "    for box in window_boxes + door_boxes:\n",
        "        box.pop(\"area\", None)  # Remove area field\n",
        "\n",
        "    # Combine all objects\n",
        "    all_objects = window_boxes + door_boxes\n",
        "\n",
        "    # Create segment directory if it doesn't exist\n",
        "    os.makedirs(segment_dir, exist_ok=True)\n",
        "\n",
        "    # Save as a pickle file in the format expected by the framework\n",
        "    segment_file_path = os.path.join(segment_dir, f\"{file_prefix}.pkl\")\n",
        "    joblib.dump(all_objects, segment_file_path)\n",
        "\n",
        "    # Save visualization image for reference\n",
        "    vis_img = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
        "    vis_img[pred == 0] = [105, 105, 105]  # Facade - Gray\n",
        "    vis_img[pred == 1] = [255, 255, 0]    # Window - Yellow\n",
        "    vis_img[pred == 2] = [139, 69, 19]    # Door - Brown\n",
        "    vis_img[pred == 3] = [220, 220, 220]  # Unknown - Light Gray\n",
        "\n",
        "    # Create segment directory if it doesn't exist\n",
        "    os.makedirs(segment_dir, exist_ok=True)\n",
        "\n",
        "    splash_path = os.path.join(segment_dir, f\"{file_prefix}_segments.png\")\n",
        "    cv2.imwrite(splash_path, vis_img[:,:,::-1])  # BGR to RGB for OpenCV\n",
        "\n",
        "    return segment_file_path, splash_path, len(window_boxes), len(door_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4vpzTYLcRyW"
      },
      "outputs": [],
      "source": [
        "# Define CityGML namespaces\n",
        "ns_citygml = \"http://www.opengis.net/citygml/2.0\"\n",
        "ns_gml = \"http://www.opengis.net/gml\"\n",
        "ns_bldg = \"http://www.opengis.net/citygml/building/2.0\"\n",
        "ns_app = \"http://www.opengis.net/citygml/appearance/2.0\"\n",
        "\n",
        "def parse_citygml(gml_file_path):\n",
        "    \"\"\"\n",
        "    Parse a CityGML file and extract building information.\n",
        "\n",
        "    Args:\n",
        "        gml_file_path: Path to the CityGML file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (buildingclasses, buildings)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Parsing CityGML file: {gml_file_path}\")\n",
        "        # Parse the CityGML file\n",
        "        CITYGML = etree.parse(gml_file_path)\n",
        "        root = CITYGML.getroot()\n",
        "\n",
        "        # Find all cityObjectMember elements\n",
        "        cityObjects = []\n",
        "        for obj in root.findall('.//{%s}cityObjectMember' % ns_citygml):\n",
        "            cityObjects.append(obj)\n",
        "\n",
        "        print(f\"\\tFound {len(cityObjects)} cityObject(s) in the CityGML file\")\n",
        "\n",
        "        # Find all building elements\n",
        "        buildings = []\n",
        "        for cityObject in cityObjects:\n",
        "            for child in cityObject.findall('.//{%s}Building' % ns_bldg):\n",
        "                buildings.append(child)\n",
        "\n",
        "        print(f\"\\tFound {len(buildings)} building(s) in the CityGML file\")\n",
        "\n",
        "        # Create building classes\n",
        "        buildingclasses = []\n",
        "        for b in buildings:\n",
        "            id = b.get('{%s}id' % ns_gml)\n",
        "            print(f\"\\tProcessing building: {id}\")\n",
        "            buildingclasses.append(readgml1.Building(b, id))\n",
        "\n",
        "        print(f\"Successfully parsed {len(buildingclasses)} buildings\")\n",
        "        return buildingclasses, buildings\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing CityGML file: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return [], []\n",
        "\n",
        "def extract_facade_ids(building):\n",
        "    \"\"\"\n",
        "    Extract facade IDs and dimensions from a building.\n",
        "\n",
        "    Args:\n",
        "        building: Building object from the parsed CityGML\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping facade IDs to surface information\n",
        "    \"\"\"\n",
        "    facade_ids = {}\n",
        "\n",
        "    # Try to extract from WallSurface elements\n",
        "    for wall in building.findall('.//{%s}WallSurface' % ns_bldg):\n",
        "        facade_id = wall.get('{%s}id' % ns_gml)\n",
        "        if facade_id:\n",
        "            facade_ids[facade_id] = {'id': facade_id}\n",
        "\n",
        "            # Try to extract dimensions from LinearRing\n",
        "            for linear_ring in wall.findall('.//{%s}LinearRing' % ns_gml):\n",
        "                pos_list = linear_ring.find('.//{%s}posList' % ns_gml)\n",
        "                if pos_list is not None and pos_list.text:\n",
        "                    # Extract coordinates\n",
        "                    coords_text = pos_list.text.strip()\n",
        "                    coords = [float(x) for x in coords_text.split()]\n",
        "\n",
        "                    # Extract basic dimensions (very simplified)\n",
        "                    if len(coords) >= 12:  # At least 4 3D points\n",
        "                        # Calculate a rough width and height\n",
        "                        x_coords = coords[0::3]  # Every 3rd element starting from 0\n",
        "                        y_coords = coords[1::3]  # Every 3rd element starting from 1\n",
        "                        z_coords = coords[2::3]  # Every 3rd element starting from 2\n",
        "\n",
        "                        width = max(np.linalg.norm(np.array([x_coords[0], y_coords[0]]) -\n",
        "                                                  np.array([x_coords[1], y_coords[1]])),\n",
        "                                   np.linalg.norm(np.array([x_coords[2], y_coords[2]]) -\n",
        "                                                  np.array([x_coords[3], y_coords[3]])))\n",
        "\n",
        "                        height = max(z_coords) - min(z_coords)\n",
        "\n",
        "                        facade_ids[facade_id]['width'] = width\n",
        "                        facade_ids[facade_id]['height'] = height\n",
        "                        facade_ids[facade_id]['area'] = width * height\n",
        "\n",
        "    return facade_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCW00vBgcTiJ"
      },
      "outputs": [],
      "source": [
        "def process_with_selective_facades(predictions, input_gml_path, specific_facades=None):\n",
        "    \"\"\"\n",
        "    Process building with given predictions for specific facades.\n",
        "\n",
        "    Args:\n",
        "        predictions: Dictionary of facade predictions with arbitrary keys\n",
        "                   {key: {'image': image_array, 'prediction': prediction_array}}\n",
        "        input_gml_path: Path to the input CityGML file\n",
        "        specific_facades: Optional list of specific facade IDs to process\n",
        "    \"\"\"\n",
        "    # Create base directories\n",
        "    base_dir = 'lod3_output'\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    segment_dir = os.path.join(base_dir, 'segments')\n",
        "    os.makedirs(segment_dir, exist_ok=True)\n",
        "    param_save_dir = os.path.join(base_dir, 'save_para')\n",
        "    os.makedirs(param_save_dir, exist_ok=True)\n",
        "    param_adjust_dir = os.path.join(base_dir, 'adjust_para')\n",
        "    os.makedirs(param_adjust_dir, exist_ok=True)\n",
        "    texture_dir = os.path.join(base_dir, 'facade_textures')\n",
        "    os.makedirs(texture_dir, exist_ok=True)\n",
        "    output_dir = os.path.join(base_dir, 'output_gml')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create the output file path with _ex suffix\n",
        "    output_gml_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(input_gml_path))[0]}_ex.gml\")\n",
        "\n",
        "    print(f\"Input GML: {input_gml_path}\")\n",
        "    print(f\"Output GML will be: {output_gml_path}\")\n",
        "\n",
        "    # Step 2: Parse CityGML to get building information\n",
        "    buildingclasses, buildings = parse_citygml(input_gml_path)\n",
        "\n",
        "    if not buildingclasses:\n",
        "        print(\"No buildings found in the CityGML file.\")\n",
        "        return\n",
        "\n",
        "    # Process each building\n",
        "    for i, (building_class, building) in enumerate(zip(buildingclasses, buildings)):\n",
        "        building_id = building_class.id\n",
        "        print(f\"\\nProcessing building: {building_id}\")\n",
        "\n",
        "        # Get all available facade IDs\n",
        "        all_facades = extract_facade_ids(building)\n",
        "\n",
        "        print(f\"Available facades for building {building_id}:\")\n",
        "        for facade_id, info in all_facades.items():\n",
        "            width = info.get('width', 'unknown')\n",
        "            height = info.get('height', 'unknown')\n",
        "            print(f\" - {facade_id} (width: {width}, height: {height})\")\n",
        "\n",
        "        # Choose which facades to process based on user input or prediction keys\n",
        "        if specific_facades:\n",
        "            # Use the specified facades if provided\n",
        "            facades_to_process = [f for f in specific_facades if f in all_facades]\n",
        "            print(f\"\\nSelectively processing {len(facades_to_process)} specified facades\")\n",
        "        else:\n",
        "            # Auto-map based on provided predictions\n",
        "            prediction_keys = list(predictions.keys())\n",
        "            real_facade_ids = list(all_facades.keys())\n",
        "\n",
        "            # If prediction keys look like they might be real facade IDs, try matching directly\n",
        "            if any('DEBY' in key for key in prediction_keys):\n",
        "                facades_to_process = [f for f in prediction_keys if f in all_facades]\n",
        "                print(f\"\\nDirectly matching {len(facades_to_process)} facade IDs from predictions\")\n",
        "            else:\n",
        "                # Otherwise, match predictions to facades in order\n",
        "                facades_to_process = real_facade_ids[:len(prediction_keys)]\n",
        "                print(f\"\\nAuto-mapping {len(facades_to_process)} predictions to facades in order\")\n",
        "\n",
        "        if not facades_to_process:\n",
        "            print(\"No facades to process! Check your facade IDs or prediction keys.\")\n",
        "            continue\n",
        "\n",
        "        # Create a mapping from real facades to predictions\n",
        "        mapped_predictions = {}\n",
        "        rel = {}  # Relationship dictionary for the framework\n",
        "\n",
        "        if specific_facades or any('DEBY' in key for key in predictions.keys()):\n",
        "            # Direct mapping - facade IDs should match prediction keys\n",
        "            for facade_id in facades_to_process:\n",
        "                if facade_id in predictions:\n",
        "                    mapped_predictions[facade_id] = predictions[facade_id]\n",
        "                    print(f\"Using prediction for facade ID '{facade_id}'\")\n",
        "                else:\n",
        "                    print(f\"No prediction found for facade ID '{facade_id}'\")\n",
        "        else:\n",
        "            # Ordered mapping - match predictions to facades in order\n",
        "            for i, facade_id in enumerate(facades_to_process):\n",
        "                if i < len(prediction_keys):\n",
        "                    pred_key = prediction_keys[i]\n",
        "                    mapped_predictions[facade_id] = predictions[pred_key]\n",
        "                    print(f\"Mapping prediction '{pred_key}' to facade ID '{facade_id}'\")\n",
        "\n",
        "        # Process each facade\n",
        "        for facade_id, pred_data in mapped_predictions.items():\n",
        "            print(f\"\\nProcessing facade: {facade_id}\")\n",
        "\n",
        "            # Clean prediction\n",
        "            pred = pred_data['prediction']\n",
        "            print(f\"Original prediction shape: {pred.shape}\")\n",
        "\n",
        "            # Clean up the prediction\n",
        "            cleaned_pred = clean_predictions(pred, min_window_area=200, min_door_area=300)\n",
        "\n",
        "            # Visualize cleaning results\n",
        "            cleaning_vis_path = os.path.join(output_dir, f\"{facade_id.replace('/', '_')}_cleaning_results.png\")\n",
        "            _, window_count, door_count = visualize_cleaning_results(pred, cleaned_pred, cleaning_vis_path)\n",
        "            print(f\"Cleaning visualization saved to: {cleaning_vis_path}\")\n",
        "\n",
        "            # Get wall dimensions from extracted facade info\n",
        "            wall_width = all_facades[facade_id].get('width', 10.0)\n",
        "            wall_height = all_facades[facade_id].get('height', 10.0)\n",
        "\n",
        "            # Scale prediction to match wall surface\n",
        "            scaled_pred = scale_prediction_to_wall_surface(cleaned_pred, wall_width, wall_height)\n",
        "            print(f\"Scaled prediction shape: {scaled_pred.shape}\")\n",
        "\n",
        "            # Save texture image for visualization\n",
        "            image = pred_data['image']\n",
        "            texture_path = os.path.join(texture_dir, f\"{building_id}_{facade_id.replace('/', '_')}.png\")\n",
        "            if hasattr(image, 'numpy'):\n",
        "                image_array = image.numpy()\n",
        "            else:\n",
        "                image_array = image\n",
        "\n",
        "            if image_array.max() <= 1.0:\n",
        "                image_array = (image_array * 255).astype(np.uint8)\n",
        "\n",
        "            cv2.imwrite(texture_path, image_array)\n",
        "\n",
        "            # Convert prediction to segment data\n",
        "            segment_path, splash_path, num_windows, num_doors = convert_prediction_to_segment_data(\n",
        "                scaled_pred, building_id, facade_id, segment_dir)\n",
        "\n",
        "            print(f\"Segment data saved to: {segment_path}\")\n",
        "            print(f\"Segment visualization saved to: {splash_path}\")\n",
        "            print(f\"Added {num_windows} windows and {num_doors} doors to facade {facade_id}\")\n",
        "\n",
        "            # Add to relationship dictionary\n",
        "            rel[facade_id] = {\n",
        "                'img': os.path.basename(texture_path),\n",
        "                'scale': 1.0  # Default scale\n",
        "            }\n",
        "\n",
        "        # Save relationship file for the LoD3Framework\n",
        "        rel_path = os.path.join(base_dir, 'img_model.pkl')\n",
        "        joblib.dump(rel, rel_path)\n",
        "        print(f\"Saved relationship file to: {rel_path}\")\n",
        "\n",
        "        # Generate LoD3 model using the framework's functions\n",
        "        print(\"\\nGenerating LoD3 model using LoD3Framework...\")\n",
        "\n",
        "        # Here you would call the appropriate functions from the LoD3Framework\n",
        "        # For example:\n",
        "        # result = creat1.run(input_gml_path, output_gml_path, base_dir)\n",
        "\n",
        "        print(f\"\\nLoD3 model generation complete!\")\n",
        "        print(f\"Output CityGML file: {output_gml_path}\")\n",
        "\n",
        "    return output_gml_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YbrLe6TXd-c4",
        "outputId": "fb33ba78-712d-4e24-9da0-222324c472bb"
      },
      "outputs": [],
      "source": [
        "# Example usage with file upload\n",
        "from google.colab import files\n",
        "\n",
        "# After running your prediction model and getting final_pred\n",
        "predictions = {\n",
        "     'DEBY_LOD2_4959457_e030de07-0827-4f7b-9657-9070e768b8f7': {'image': original_image, 'prediction': final_pred}\n",
        "#    ,'facade2': {'image': original_image2, 'prediction': final_pred2} Add more facades as needed\n",
        "}\n",
        "\n",
        "# Upload CityGML file\n",
        "print(\"Please upload your CityGML file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"No file was uploaded.\")\n",
        "else:\n",
        "    # Get the first uploaded file\n",
        "    filename = next(iter(uploaded.keys()))\n",
        "    input_gml_path = os.path.join(BASE_DIR, filename)\n",
        "\n",
        "    # Move the file to the right location\n",
        "    with open(input_gml_path, 'wb') as f:\n",
        "        f.write(uploaded[filename])\n",
        "\n",
        "    print(f\"File saved to: {input_gml_path}\")\n",
        "\n",
        "    # Process the building (with or without specific facade selection)\n",
        "    output_gml_path = process_with_selective_facades(\n",
        "        predictions,\n",
        "        input_gml_path,\n",
        "        specific_facades=['DEBY_LOD2_4959457_e030de07-0827-4f7b-9657-9070e768b8f7']\n",
        "        # Remove the specific_facades parameter or provide your own list\n",
        "    )\n",
        "\n",
        "    print(f\"Final output: {output_gml_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
