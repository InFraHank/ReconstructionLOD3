{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWKGnw6dmqLG"
      },
      "source": [
        "# Scan2LOD Predictor\n",
        "### Prediction Logic that Classifies Facade Data [Images and Conflictmaps (3D-Data)] of the same building with the help of Convolutional Neural Networks.\n",
        "\n",
        "------------------------------------------------------------------------------\n",
        "## Input: 572x572 image (.jpg) and 572x572 conflict map (.png)\n",
        "------------------------------------------------------------------------------\n",
        "## Output: Class for each individual pixel\n",
        "\n",
        "The classes that are being output are:\n",
        "\n",
        "-Facade\n",
        "\n",
        "-Window\n",
        "\n",
        "-Door\n",
        "\n",
        "-Unknown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6n67QtRsc6O"
      },
      "source": [
        "# 1. IMPORTING LIBRARIES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftSvkOJTofOY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.utils import save_image\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm  # For progress bars\n",
        "import time\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An0cNdA3ktXn",
        "outputId": "5ed13d86-e9df-4b4e-fd88-beebbad6d80f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3PWRDJpsZzu"
      },
      "source": [
        "# 2. SETTING UP DIRECTORIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF0hXzMio3nU",
        "outputId": "209e47da-69c4-41f3-8240-6bb32080ac77"
      },
      "outputs": [],
      "source": [
        "# base directory\n",
        "BASE_DIR = '/content/drive/MyDrive/Colab Notebooks/Scan2LOD_DATA'\n",
        "\n",
        "# Folders\n",
        "INPUT_DIR = os.path.join(BASE_DIR, 'Inputs')\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'Models')\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'Outputs')\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Define paths for specific types of inputs\n",
        "CONFLICT_MAP_DIR = os.path.join(INPUT_DIR, 'ConflictMaps')\n",
        "IMAGE_DIR = os.path.join(INPUT_DIR, 'FacadeImages')\n",
        "GROUND_TRUTH_DIR = os.path.join(INPUT_DIR, 'GroundTruth')  # If available\n",
        "\n",
        "# Define paths for model files\n",
        "UNET_MODEL_PATH = os.path.join(MODEL_DIR, 'unet_model.pth')\n",
        "MASKRCNN_MODEL_PATH = os.path.join(MODEL_DIR, 'maskrcnn_model.pth')\n",
        "\n",
        "# Create output subdirectories\n",
        "UNET_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'UNet')\n",
        "MASKRCNN_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'MaskRCNN')\n",
        "FUSION_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'Fusion')\n",
        "\n",
        "os.makedirs(UNET_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MASKRCNN_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FUSION_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Directory structure initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E44Y_vqNjRas"
      },
      "outputs": [],
      "source": [
        "# Adjust to change\n",
        "# FacadeImage and conflict map paths\n",
        "\n",
        "CONF_MAP_PATH = os.path.join(INPUT_DIR, 'ConflictMaps', 'Building57_ConflictMap.png')\n",
        "predictionPath = os.path.join(IMAGE_DIR, 'Building57_FacadeImage.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBmz-ircsyWt"
      },
      "source": [
        "# 3. MODEL ARCHITECTURES (U-Net and MaskRCNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyNXTzZHsUD3"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding='same')  # Using 'same' padding\n",
        "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
        "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
        "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding='same')\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding='same')\n",
        "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding='same')\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding='same')\n",
        "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding='same')\n",
        "\n",
        "        # Decoder with size-matching upconvolution\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding='same')\n",
        "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding='same')\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding='same')\n",
        "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding='same')\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding='same')\n",
        "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding='same')\n",
        "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
        "\n",
        "        # Output layer\n",
        "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        xe11 = F.relu(self.e11(x))\n",
        "        xe12 = F.relu(self.e12(xe11))\n",
        "        xp1 = self.pool1(xe12)\n",
        "\n",
        "        xe21 = F.relu(self.e21(xp1))\n",
        "        xe22 = F.relu(self.e22(xe21))\n",
        "        xp2 = self.pool2(xe22)\n",
        "\n",
        "        xe31 = F.relu(self.e31(xp2))\n",
        "        xe32 = F.relu(self.e32(xe31))\n",
        "        xp3 = self.pool3(xe32)\n",
        "\n",
        "        xe41 = F.relu(self.e41(xp3))\n",
        "        xe42 = F.relu(self.e42(xe41))\n",
        "        xp4 = self.pool4(xe42)\n",
        "\n",
        "        xe51 = F.relu(self.e51(xp4))\n",
        "        xe52 = F.relu(self.e52(xe51))\n",
        "\n",
        "        # Decoder with size checking\n",
        "        xu1 = self.upconv1(xe52)\n",
        "        if xu1.size() != xe42.size():\n",
        "            xu1 = F.interpolate(xu1, size=xe42.size()[2:])\n",
        "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
        "        xd11 = F.relu(self.d11(xu11))\n",
        "        xd12 = F.relu(self.d12(xd11))\n",
        "\n",
        "        xu2 = self.upconv2(xd12)\n",
        "        if xu2.size() != xe32.size():\n",
        "            xu2 = F.interpolate(xu2, size=xe32.size()[2:])\n",
        "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
        "        xd21 = F.relu(self.d21(xu22))\n",
        "        xd22 = F.relu(self.d22(xd21))\n",
        "\n",
        "        xu3 = self.upconv3(xd22)\n",
        "        if xu3.size() != xe22.size():\n",
        "            xu3 = F.interpolate(xu3, size=xe22.size()[2:])\n",
        "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
        "        xd31 = F.relu(self.d31(xu33))\n",
        "        xd32 = F.relu(self.d32(xd31))\n",
        "\n",
        "        xu4 = self.upconv4(xd32)\n",
        "        if xu4.size() != xe12.size():\n",
        "            xu4 = F.interpolate(xu4, size=xe12.size()[2:])\n",
        "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
        "        xd41 = F.relu(self.d41(xu44))\n",
        "        xd42 = F.relu(self.d42(xd41))\n",
        "\n",
        "        # Output layer\n",
        "        out = self.outconv(xd42)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNOtgXT6Saoc"
      },
      "outputs": [],
      "source": [
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy86T4_RtEji"
      },
      "source": [
        "# 4. LOADING MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK_C153dNGbJ"
      },
      "outputs": [],
      "source": [
        "def load_unet_model(model_path, num_classes=4, device='cuda'):\n",
        "    \"\"\"Load a trained U-Net model\"\"\"\n",
        "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "    model = UNet(n_class=num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(f\"U-Net model loaded from: {model_path}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khtyULqAtF_i"
      },
      "outputs": [],
      "source": [
        "def load_maskrcnn_model(model_path, device='cuda'):\n",
        "    \"\"\"\n",
        "    Load a trained Mask R-CNN model\n",
        "    \"\"\"\n",
        "    # This is a placeholder - adjust based on your Mask R-CNN architecture\n",
        "    try:\n",
        "\n",
        "        # Initialize model with pre-trained weights\n",
        "        num_classes = 2\n",
        "        device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "        model = get_instance_segmentation_model(num_classes)\n",
        "        weights = torch.load(model_path)\n",
        "\n",
        "        # Load your trained weights\n",
        "        model.load_state_dict(weights)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        print(f\"Mask R-CNN model loaded from: {model_path}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Mask R-CNN model: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_fEpazkOy1O"
      },
      "outputs": [],
      "source": [
        "def get_probabilities(self, x):\n",
        "    \"\"\"Get both raw logits and normalized probabilities\"\"\"\n",
        "    logits = self.forward(x)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return logits, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ImnFqR8OhPC"
      },
      "outputs": [],
      "source": [
        "def preprocess_conflict_map(conflict_map_path, device):\n",
        "    \"\"\"Preprocess a conflict map for the U-Net model\"\"\"\n",
        "    # Color mappings for conflict map\n",
        "    CONFLICT_MAP_COLORS = {\n",
        "        'confirming': (0, 255, 0),  # Green - Confirming\n",
        "        'unknown': (0, 0, 255),     # Blue - Unknown\n",
        "        'conflict': (255, 0, 0)     # Red - Conflict\n",
        "    }\n",
        "\n",
        "    # Load conflict map as RGB\n",
        "    conf_map = Image.open(conflict_map_path).convert('RGB')\n",
        "    conf_map_array = np.array(conf_map)\n",
        "\n",
        "    # Create input channels based on different conflict map states\n",
        "    confirming_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['confirming'], axis=2)\n",
        "    unknown_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['unknown'], axis=2)\n",
        "    conflict_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['conflict'], axis=2)\n",
        "\n",
        "    # Stack the masks into a 3-channel tensor\n",
        "    conf_map_tensor = np.stack([confirming_mask, unknown_mask, conflict_mask], axis=0)\n",
        "    conf_map_tensor = torch.from_numpy(conf_map_tensor).float().unsqueeze(0).to(device)\n",
        "\n",
        "    return conf_map_tensor, conf_map_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OphrfVLgOlTE"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, device, target_size=(572, 572)):\n",
        "    # Load and normalize image for Mask R-CNN\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize(target_size, Image.LANCZOS)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    return image_tensor, np.array(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D73_0z53Razj"
      },
      "outputs": [],
      "source": [
        "def preprocess_conflict_map(conflict_map_path, device):\n",
        "    \"\"\"Preprocess a conflict map for the U-Net model\"\"\"\n",
        "    # Color mappings for conflict map\n",
        "    CONFLICT_MAP_COLORS = {\n",
        "        'confirming': (0, 255, 0),  # Green - Confirming\n",
        "        'unknown': (0, 0, 255),     # Blue - Unknown\n",
        "        'conflict': (255, 0, 0)     # Red - Conflict\n",
        "    }\n",
        "\n",
        "    # Load conflict map as RGB\n",
        "    conf_map = Image.open(conflict_map_path).convert('RGB')\n",
        "    conf_map_array = np.array(conf_map)\n",
        "\n",
        "    # Create input channels based on different conflict map states\n",
        "    confirming_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['confirming'], axis=2)\n",
        "    unknown_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['unknown'], axis=2)\n",
        "    conflict_mask = np.all(conf_map_array == CONFLICT_MAP_COLORS['conflict'], axis=2)\n",
        "\n",
        "    # Stack the masks into a 3-channel tensor\n",
        "    conf_map_tensor = np.stack([confirming_mask, unknown_mask, conflict_mask], axis=0)\n",
        "    conf_map_tensor = torch.from_numpy(conf_map_tensor).float().unsqueeze(0).to(device)\n",
        "\n",
        "    return conf_map_tensor, conf_map_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjX9h9FGR7ye"
      },
      "outputs": [],
      "source": [
        "def test_unet_output(model, conflict_map_path, device='cuda'):\n",
        "    \"\"\"Test UNet model on a conflict map and visualize outputs\"\"\"\n",
        "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Preprocess conflict map\n",
        "    conf_map_tensor, conf_map_array = preprocess_conflict_map(conflict_map_path, device)\n",
        "\n",
        "    # Forward pass through UNet\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get raw logits from model\n",
        "        logits = model(conf_map_tensor)\n",
        "\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Get class predictions\n",
        "        pred_classes = torch.argmax(logits, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "        # Convert to numpy for visualization\n",
        "        logits_np = logits.squeeze().cpu().numpy()\n",
        "        probs_np = probs.squeeze().cpu().numpy()\n",
        "\n",
        "    # Define class names for visualization\n",
        "    class_names = ['Facade', 'Window', 'Door', 'Unknown']\n",
        "\n",
        "    # Visualize input\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(conf_map_array)\n",
        "    plt.title('Input Conflict Map')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize raw logits\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.imshow(logits_np[i], cmap='viridis')\n",
        "        plt.colorbar()\n",
        "        plt.title(f'Raw Logits: {class_name}')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize probabilities\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.imshow(probs_np[i], cmap='viridis', vmin=0, vmax=1)\n",
        "        plt.colorbar()\n",
        "        plt.title(f'Probability: {class_name}')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize predicted class\n",
        "    class_colors = {\n",
        "        0: [105, 105, 105],  # Facade - Gray\n",
        "        1: [255, 255, 0],    # Window - Yellow\n",
        "        2: [139, 69, 19],    # Door - Brown\n",
        "        3: [220, 220, 220]   # Unknown - Light Gray\n",
        "    }\n",
        "\n",
        "    # Convert prediction to RGB\n",
        "    rgb_pred = np.zeros((pred_classes.shape[0], pred_classes.shape[1], 3), dtype=np.uint8)\n",
        "    for class_idx, color in class_colors.items():\n",
        "        rgb_pred[pred_classes == class_idx] = color\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(rgb_pred)\n",
        "    plt.title('UNet Prediction')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics for each class\n",
        "    print(\"Class probability statistics:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"\\n{class_name}:\")\n",
        "        print(f\"  Min: {probs_np[i].min():.4f}\")\n",
        "        print(f\"  Max: {probs_np[i].max():.4f}\")\n",
        "        print(f\"  Mean: {probs_np[i].mean():.4f}\")\n",
        "        print(f\"  Pixels predicted as {class_name}: {np.sum(pred_classes == i)} ({np.sum(pred_classes == i)/pred_classes.size*100:.2f}%)\")\n",
        "\n",
        "    # Return processed data for further analysis\n",
        "    return {\n",
        "        'raw_logits': logits_np,\n",
        "        'probabilities': probs_np,\n",
        "        'predictions': pred_classes\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjADFmIaO-S-"
      },
      "source": [
        "## 5. Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnI-J2okPET8"
      },
      "outputs": [],
      "source": [
        "def get_unet_predictions(model, conflict_map_tensor):\n",
        "    with torch.no_grad():\n",
        "        logits, probabilities = model.get_probabilities(conflict_map_tensor)\n",
        "        predictions = torch.argmax(logits, dim=1).squeeze().cpu().numpy()\n",
        "        probabilities = probabilities.squeeze().cpu().numpy()\n",
        "\n",
        "    return predictions, probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "160EW4VQSDJF",
        "outputId": "f87a0779-b63e-4ddf-d7b8-5ca2bd8a6632"
      },
      "outputs": [],
      "source": [
        "UNET_PATH = os.path.join(MODEL_DIR, 'Facade_model_b14_e50_CMPGENREAL_nS.pth')\n",
        "\n",
        "# Load model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "unet_model = load_unet_model(UNET_PATH, num_classes=4, device=device)\n",
        "\n",
        "# Test model output\n",
        "unet_results = test_unet_output(unet_model, CONF_MAP_PATH, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdcmOk8wVfmk",
        "outputId": "c35a8c4c-a791-48b7-8284-52e16716f1ed"
      },
      "outputs": [],
      "source": [
        "device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "MaskRCNN_windows_model = load_maskrcnn_model(os.path.join(MODEL_DIR, 'MASK_RCNN_onlyWindows.pth'))\n",
        "MaskRCNN_doors_model = load_maskrcnn_model(os.path.join(MODEL_DIR, 'MASK_RCNN_doors.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrIl6peohnSM"
      },
      "outputs": [],
      "source": [
        "predImg = Image.open(predictionPath).convert(\"RGB\")\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "img_tensor = transform(predImg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "collapsed": true,
        "id": "FbmnSryFhO0U",
        "outputId": "a4a3e811-49d7-450a-f06f-8a3f5c03f870"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(img_tensor.mul(255).permute(1, 2, 0).byte().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzVz-ZKq9KwJ"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    prediction = MaskRCNN_windows_model([img_tensor.to(device)])\n",
        "selected_photo = prediction[0]\n",
        "selected_masks = selected_photo['masks']\n",
        "\n",
        "#squeeze extra dimension given to the masks\n",
        "selected_masks = torch.squeeze(selected_masks, 1)\n",
        "\n",
        "#test = selected_masks[1] #first element of the tensor = first instance mask\n",
        "num_of_masks = selected_masks.size(dim=0)\n",
        "x_of_masks = selected_masks.size(dim=1)\n",
        "y_of_masks = selected_masks.size(dim=2)\n",
        "\n",
        "list_of_arrays = []\n",
        "final_mask_windows = selected_masks.sum(axis=0)\n",
        "for i in range(num_of_masks):\n",
        "  np_arr = selected_masks[i].cpu().detach().numpy()\n",
        "  list_of_arrays.append(np_arr)\n",
        "\n",
        "one_composite_mask_windows = sum(list_of_arrays)\n",
        "\n",
        "# save_path = path + \"/results/wallBmaskFull.jpg\"\n",
        "# save_image(selected_masks, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2p3ADXfi4qD"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    prediction = MaskRCNN_doors_model([img_tensor.to(device)])\n",
        "selected_photo = prediction[0]\n",
        "selected_masks = selected_photo['masks']\n",
        "\n",
        "#squeeze extra dimension given to the masks\n",
        "selected_masks = torch.squeeze(selected_masks, 1)\n",
        "\n",
        "#test = selected_masks[1] #first element of the tensor = first instance mask\n",
        "num_of_masks = selected_masks.size(dim=0)\n",
        "x_of_masks = selected_masks.size(dim=1)\n",
        "y_of_masks = selected_masks.size(dim=2)\n",
        "\n",
        "list_of_arrays = []\n",
        "final_mask_doors = selected_masks.sum(axis=0)\n",
        "for i in range(num_of_masks):\n",
        "  np_arr = selected_masks[i].cpu().detach().numpy()\n",
        "  list_of_arrays.append(np_arr)\n",
        "\n",
        "one_composite_mask_doors = sum(list_of_arrays)\n",
        "\n",
        "# save_path = path + \"/results/wallBmaskFull.jpg\"\n",
        "# save_image(selected_masks, save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d9m-hHfRhsu-",
        "outputId": "14b32f90-1d1c-4a49-d396-149d9714efcd"
      },
      "outputs": [],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "BIi2nNb8jNZ0",
        "outputId": "0d225e17-64e0-442a-bbc8-b7c6007ae870"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(final_mask_windows.mul(255).byte().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "riXf6uqI-Dk9",
        "outputId": "88c86406-c78d-414a-fec1-0d1dce2cfe69"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(final_mask_doors.mul(255).byte().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP57gVjDwugd"
      },
      "outputs": [],
      "source": [
        "def normalize_maskrcnn_output(mask, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Normalize Mask R-CNN output to scale pixel values between 0 and 1.\n",
        "    Simply finds global min and max and scales all values.\n",
        "\n",
        "    Parameters:\n",
        "    mask: Instance masks from Mask R-CNN\n",
        "    epsilon: Small value to prevent division by zero\n",
        "\n",
        "    Returns:\n",
        "    Normalized probability tensor\n",
        "    \"\"\"\n",
        "    # Move tensor to CPU and convert to numpy\n",
        "    logits = mask.cpu().numpy()\n",
        "\n",
        "    # Get the global min and max across the entire array, regardless of shape\n",
        "    min_val = np.min(logits)\n",
        "    max_val = np.max(logits)\n",
        "\n",
        "    # Simple min-max normalization\n",
        "    range_val = max_val - min_val\n",
        "    if range_val < epsilon:\n",
        "        # If there's no variation, return zeros in the original shape\n",
        "        return np.zeros_like(logits)\n",
        "\n",
        "    # Normalize to [0,1] range\n",
        "    normalized = (logits - min_val) / range_val\n",
        "\n",
        "    # If we need a single 2D image as output, take the max across instance dimension if it exists\n",
        "    if len(normalized.shape) == 3:\n",
        "        normalized = np.max(normalized, axis=0)\n",
        "\n",
        "    return normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAB4BvK49imD"
      },
      "outputs": [],
      "source": [
        "## Unet output\n",
        "unet_probs = unet_results['probabilities']  # Shape [4, height, width]\n",
        "#unet_pred = unet_results['predictions']    # Shape [height, width]\n",
        "\n",
        "## MaskRCNN outputs\n",
        "window_probs = normalize_maskrcnn_output(final_mask_windows)\n",
        "window_probs.reshape((572, 572))\n",
        "\n",
        "#windows_blinds_probs = normalize_maskrcnn_output(final_mask_windows_blinds)\n",
        "#windows_blinds_probs.reshape((572, 572))\n",
        "\n",
        "#door_probs = window_probs + windows_blinds_probs\n",
        "\n",
        "\n",
        "\n",
        "# Apply the condition using NumPy's boolean indexing\n",
        "#door_probs[door_probs < 1] = 0  # Set values above X to zero\n",
        "\n",
        "door_probs = normalize_maskrcnn_output(final_mask_doors)\n",
        "\n",
        "# Renormalize after addition\n",
        "if np.max(door_probs) > 0:\n",
        "    door_probs = door_probs / np.max(door_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "3Olg5iDR7gFM",
        "outputId": "eeb8c764-5fca-4ce9-8c59-215073048322"
      },
      "outputs": [],
      "source": [
        "# Plot the heatmap\n",
        "plt.figure(figsize=(6, 6))\n",
        "#plt.imshow(unet_probs[1], cmap=\"viridis\", interpolation=\"nearest\")\n",
        "#plt.imshow(window_probs, cmap=\"viridis\", interpolation=\"nearest\")\n",
        "\n",
        "\n",
        "\n",
        "plt.imshow(final_mask_windows.mul(255).byte().cpu().numpy()/255, cmap=\"viridis\", interpolation=\"nearest\")\n",
        "\n",
        "plt.colorbar(label=\"Probability Value\")\n",
        "plt.title(\"Visualization of window_probs Array\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1XVMBK60B51"
      },
      "source": [
        "## 6. Semantic Fusion\n",
        "\n",
        "The fusion of the predictions happens with a set of hyperparameters that form our final estimate by weighting the estimates of Windows and Doors segmented by our Unet with the estimates from the MaskRCNN. Facade and Unknown classes will be directly taken from the UNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTVWbC9eFEdg"
      },
      "outputs": [],
      "source": [
        "def combine_predictions_linear(unet_probs, window_probs, door_probs):\n",
        "    \"\"\"\n",
        "    Create a linear weighted combination of UNet and Mask R-CNN predictions\n",
        "\n",
        "    Parameters:\n",
        "        unet_probs: UNet probabilities [4, height, width] - all classes\n",
        "        window_probs: Window probabilities from Mask R-CNN [height, width]\n",
        "        door_probs: Door probabilities from Mask R-CNN [height, width], optional\n",
        "        window_weight: Weight for Mask R-CNN window prediction (1-window_weight for UNet)\n",
        "        door_weight: Weight for Mask R-CNN door prediction (1-door_weight for UNet)\n",
        "\n",
        "    Returns:\n",
        "        combined_probs: Combined probability tensor [4, height, width]\n",
        "    \"\"\"\n",
        "    # Create a copy of UNet probabilities to avoid modifying original\n",
        "    combined_probs = unet_probs.copy()\n",
        "\n",
        "    # For window class [1], apply weighted combination\n",
        "    if window_probs is not None:\n",
        "        window_alpha = 3\n",
        "        window_beta = 3 #3 was valid for 57 // 6 was valid for 57_2\n",
        "        combined_probs[1] = window_alpha * unet_probs[1] + window_beta * window_probs\n",
        "\n",
        "    # For door class [2], apply weighted combination if available\n",
        "    if door_probs is not None:\n",
        "        door_alpha = 4.5\n",
        "        door_beta = 1\n",
        "        combined_probs[2] = door_alpha * unet_probs[2] + door_beta * door_probs\n",
        "\n",
        "    # Unknowns\n",
        "    combined_probs[3] = 6* combined_probs[3]\n",
        "\n",
        "    # Normalize probabilities to sum to 1 for each pixel\n",
        "    sum_probs = np.sum(combined_probs, axis=0)\n",
        "    for i in range(combined_probs.shape[0]):\n",
        "        combined_probs[i] /= sum_probs\n",
        "\n",
        "    return combined_probs\n",
        "\n",
        "def adaptive_combine_predictions(unet_probs, window_probs, door_probs=None,\n",
        "                               window_threshold=0.3, door_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Create an adaptive weighted combination based on UNet confidence\n",
        "\n",
        "    Parameters:\n",
        "        unet_probs: UNet probabilities [4, height, width] - all classes\n",
        "        window_probs: Window probabilities from Mask R-CNN [height, width]\n",
        "        door_probs: Door probabilities from Mask R-CNN [height, width], optional\n",
        "        window_threshold: Confidence threshold for windows\n",
        "        door_threshold: Confidence threshold for doors\n",
        "\n",
        "    Returns:\n",
        "        combined_probs: Combined probability tensor [4, height, width]\n",
        "    \"\"\"\n",
        "    # Create a copy of UNet probabilities to avoid modifying original\n",
        "    combined_probs = unet_probs.copy()\n",
        "\n",
        "\n",
        "    # Calculate UNet confidence for each class\n",
        "    window_confidence = unet_probs[1]\n",
        "    door_confidence = unet_probs[2]\n",
        "\n",
        "    # WINDOWS: use Mask R-CNN more where UNet is less confident\n",
        "    if window_probs is not None:\n",
        "        # Calculate adaptive weights based on confidence\n",
        "        window_weight = np.where(window_confidence < window_threshold, 0.8, 0.2)\n",
        "\n",
        "        # Apply weighted combination\n",
        "        combined_probs[1] = (1 - window_weight) * unet_probs[1] + window_weight * window_probs\n",
        "\n",
        "    # DOORS: use Mask R-CNN more where UNet is less confident\n",
        "    if door_probs is not None:\n",
        "        # Calculate adaptive weights based on confidence\n",
        "        door_weight = np.where(door_confidence < door_threshold, 0.8, 0.2)\n",
        "\n",
        "        # Apply weighted combination\n",
        "        combined_probs[2] = (1 - door_weight) * unet_probs[2] + door_weight * door_probs\n",
        "\n",
        "    # Normalize probabilities to sum to 1 for each pixel\n",
        "    sum_probs = np.sum(combined_probs, axis=0)\n",
        "    for i in range(combined_probs.shape[0]):\n",
        "        combined_probs[i] /= sum_probs\n",
        "\n",
        "    return combined_probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-tEiONmHRDG"
      },
      "outputs": [],
      "source": [
        "combined_probs = combine_predictions_linear(\n",
        "     unet_probs,            # From UNet\n",
        "     window_probs,          # From Window Mask R-CNN\n",
        "     door_probs\n",
        " )\n",
        "\n",
        "#combined_probs = adaptive_combine_predictions(\n",
        " #    unet_probs,            # From UNet\n",
        " #    window_probs,          # From Window Mask R-CNN\n",
        " #    door_probs,            # From Door Mask R-CNN (optional)\n",
        " #    window_threshold=0.4,\n",
        " #    door_threshold=0.4\n",
        " #)\n",
        "\n",
        " # Get final prediction\n",
        "final_pred = np.argmax(combined_probs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIdOfe9DH05a"
      },
      "source": [
        "## 7. Final Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFi7J9ROHsPL"
      },
      "outputs": [],
      "source": [
        "def visualize_final_prediction(pred, original_image=None, save_path=None, show_plot=True):\n",
        "    \"\"\"\n",
        "    Visualize the final prediction with optional comparison to inputs and individual model outputs\n",
        "\n",
        "    Parameters:\n",
        "        pred: Final prediction array of shape [height, width] with class indices\n",
        "        original_image: Original input image (optional)\n",
        "        unet_pred: UNet prediction array (optional)\n",
        "        maskrcnn_pred: Mask R-CNN prediction array (optional)\n",
        "        save_path: Path to save the visualization (optional)\n",
        "        show_plot: Whether to display the plot (default True)\n",
        "    \"\"\"\n",
        "    # Define class colors\n",
        "    class_colors = {\n",
        "        0: [105, 105, 105],  # Facade - Gray\n",
        "        1: [255, 255, 0],    # Window - Yellow\n",
        "        2: [139, 69, 19],    # Door - Brown\n",
        "        3: [220, 220, 220]   # Unknown - Light Gray\n",
        "    }\n",
        "\n",
        "    # Function to convert prediction to RGB\n",
        "    def pred_to_rgb(pred_array):\n",
        "        height, width = pred_array.shape\n",
        "        rgb = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "        for class_idx, color in class_colors.items():\n",
        "            mask = (pred_array == class_idx)\n",
        "            rgb[mask] = color\n",
        "        return rgb\n",
        "\n",
        "    # Convert final prediction to RGB\n",
        "    rgb_pred = pred_to_rgb(pred)\n",
        "\n",
        "    # Determine the number of subplots needed\n",
        "    num_plots = 1\n",
        "    if original_image is not None:\n",
        "        num_plots += 1\n",
        "\n",
        "    # Create figure with appropriate size\n",
        "    fig = plt.figure(figsize=(5*num_plots, 5))\n",
        "\n",
        "    # Plot counter\n",
        "    plot_idx = 1\n",
        "\n",
        "    # Original image\n",
        "    if original_image is not None:\n",
        "        plt.subplot(1, num_plots, plot_idx)\n",
        "        plt.imshow(original_image)\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis('off')\n",
        "        plot_idx += 1\n",
        "\n",
        "    # Final prediction\n",
        "    plt.subplot(1, num_plots, plot_idx)\n",
        "    plt.imshow(rgb_pred)\n",
        "    plt.title(\"Final Prediction\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Add a legend\n",
        "    legend_elements = []\n",
        "    class_names = [\"Facade\", \"Window\", \"Door\", \"Unknown\"]\n",
        "    for i, (class_idx, color) in enumerate(class_colors.items()):\n",
        "        # Convert RGB to matplotlib format (0-1)\n",
        "        mpl_color = [c/255 for c in color]\n",
        "        legend_elements.append(plt.Rectangle((0,0), 1, 1, color=mpl_color, label=class_names[i]))\n",
        "\n",
        "    # Add the legend to the right side of the plot\n",
        "    fig.legend(handles=legend_elements, loc='center right', title=\"Classes\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout for legend\n",
        "\n",
        "    # Save if requested\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Visualization saved to {save_path}\")\n",
        "\n",
        "    # Show if requested\n",
        "    if show_plot:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "    return rgb_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i90MSvIdIC9T",
        "outputId": "453337f9-4607-4031-98b2-4e5bad552b22"
      },
      "outputs": [],
      "source": [
        "# Get the final prediction\n",
        "final_pred = np.argmax(combined_probs, axis=0)\n",
        "\n",
        "# Visualize with comparisons\n",
        "original_image = np.array(Image.open(predictionPath).convert(\"RGB\"))\n",
        "visualize_final_prediction(\n",
        "    final_pred,\n",
        "    original_image=original_image,\n",
        "    save_path=os.path.join(OUTPUT_DIR, \"fusion_result.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3offsn6A0pdw"
      },
      "source": [
        "#3D Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ojbJoq6UHR"
      },
      "source": [
        "# 8. Reconstruction with WallMatching\n",
        "\n",
        "- export unter LOD3 output\n",
        "- brauchen die Facade ID von der Frontfacade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M03Y3oKWpPT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from lxml import etree\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqMLTIMGWvle"
      },
      "outputs": [],
      "source": [
        "# Define CityGML namespaces\n",
        "ns_citygml = \"http://www.opengis.net/citygml/2.0\"\n",
        "ns_gml = \"http://www.opengis.net/gml\"\n",
        "ns_bldg = \"http://www.opengis.net/citygml/building/2.0\"\n",
        "ns_app = \"http://www.opengis.net/citygml/appearance/2.0\"\n",
        "\n",
        "def extract_wall_surfaces(gml_tree):\n",
        "    \"\"\"Extract all wall surfaces from a CityGML file\"\"\"\n",
        "    wall_surfaces = []\n",
        "    for wall in gml_tree.findall(f\".//{{{ns_bldg}}}WallSurface\"):\n",
        "        wall_id = wall.get(f\"{{{ns_gml}}}id\")\n",
        "        # Find wall polygons to get coordinates\n",
        "        polygons = []\n",
        "        for polygon in wall.findall(f\".//{{{ns_gml}}}Polygon\"):\n",
        "            pos_list = polygon.find(f\".//{{{ns_gml}}}posList\")\n",
        "            if pos_list is not None and pos_list.text:\n",
        "                coords_text = pos_list.text.strip()\n",
        "                coords = [float(x) for x in coords_text.split()]\n",
        "                polygons.append({\n",
        "                    \"id\": polygon.get(f\"{{{ns_gml}}}id\"),\n",
        "                    \"coords\": coords\n",
        "                })\n",
        "\n",
        "        wall_surfaces.append({\n",
        "            \"id\": wall_id,\n",
        "            \"element\": wall,\n",
        "            \"polygons\": polygons\n",
        "        })\n",
        "\n",
        "    return wall_surfaces\n",
        "\n",
        "def calculate_wall_dimensions(coords):\n",
        "    \"\"\"Calculate approximate width and height of a wall from coordinates\"\"\"\n",
        "    # Convert flat coordinate list to points\n",
        "    points = []\n",
        "    for i in range(0, len(coords), 3):\n",
        "        if i+2 < len(coords):\n",
        "            points.append((coords[i], coords[i+1], coords[i+2]))\n",
        "\n",
        "    if len(points) < 4:\n",
        "        return 1, 1  # Default fallback\n",
        "\n",
        "    # Find min and max z values (height)\n",
        "    z_values = [p[2] for p in points]\n",
        "    height = max(z_values) - min(z_values)\n",
        "\n",
        "    # For width, use horizontal distance between points\n",
        "    # (simplified - just using the first and second points as estimate)\n",
        "    width = np.sqrt((points[1][0] - points[0][0])**2 + (points[1][1] - points[0][1])**2)\n",
        "\n",
        "    return width, height\n",
        "\n",
        "def find_wall_orientation(coords):\n",
        "    \"\"\"Find the orientation of a wall to position openings correctly\"\"\"\n",
        "    points = []\n",
        "    for i in range(0, len(coords), 3):\n",
        "        if i+2 < len(coords):\n",
        "            points.append((coords[i], coords[i+1], coords[i+2]))\n",
        "\n",
        "    if len(points) < 4:\n",
        "        return (1, 0, 0)  # Default fallback\n",
        "\n",
        "    # Calculate normal vector (simplified)\n",
        "    # Using the first three points to determine orientation\n",
        "    p0 = np.array(points[0])\n",
        "    p1 = np.array(points[1])\n",
        "    p2 = np.array(points[2])\n",
        "\n",
        "    v1 = p1 - p0\n",
        "    v2 = p2 - p1\n",
        "\n",
        "    # Cross product gives normal vector\n",
        "    normal = np.cross(v1, v2)\n",
        "\n",
        "    # Normalize\n",
        "    length = np.linalg.norm(normal)\n",
        "    if length > 0:\n",
        "        normal = normal / length\n",
        "\n",
        "    return normal\n",
        "\n",
        "def generate_opening_coords(wall_coords, relative_position, size, opening_type):\n",
        "    \"\"\"\n",
        "    Generate 3D coordinates for an opening on a wall\n",
        "\n",
        "    Parameters:\n",
        "    - wall_coords: List of wall coordinates\n",
        "    - relative_position: (x, y) position in [0,1] range relative to wall dimensions\n",
        "    - size: (width, height) in real-world units\n",
        "    - opening_type: \"window\" or \"door\"\n",
        "\n",
        "    Returns:\n",
        "    - List of 3D coordinates for the opening\n",
        "    \"\"\"\n",
        "    # Convert flat coordinate list to points\n",
        "    points = []\n",
        "    for i in range(0, len(wall_coords), 3):\n",
        "        if i+2 < len(wall_coords):\n",
        "            points.append((wall_coords[i], wall_coords[i+1], wall_coords[i+2]))\n",
        "\n",
        "    if len(points) < 4:\n",
        "        return []  # Not enough points\n",
        "\n",
        "    # Find min/max values for each dimension\n",
        "    x_values = [p[0] for p in points]\n",
        "    y_values = [p[1] for p in points]\n",
        "    z_values = [p[2] for p in points]\n",
        "\n",
        "    x_min, x_max = min(x_values), max(x_values)\n",
        "    y_min, y_max = min(y_values), max(y_values)\n",
        "    z_min, z_max = min(z_values), max(z_values)\n",
        "\n",
        "    # Calculate wall dimensions\n",
        "    width = max(np.sqrt((x_max - x_min)**2 + (y_max - y_min)**2), 0.1)\n",
        "    height = max(z_max - z_min, 0.1)\n",
        "\n",
        "    # Find orientation\n",
        "    orientation = find_wall_orientation(wall_coords)\n",
        "\n",
        "    # Calculate the wall's primary direction vector\n",
        "    # (vector between first two points)\n",
        "    if len(points) >= 2:\n",
        "        direction = np.array([points[1][0] - points[0][0],\n",
        "                            points[1][1] - points[0][1],\n",
        "                            0])  # Keep it horizontal\n",
        "        direction_length = np.linalg.norm(direction)\n",
        "        if direction_length > 0:\n",
        "            direction = direction / direction_length\n",
        "    else:\n",
        "        direction = np.array([1, 0, 0])  # Default\n",
        "\n",
        "    # Default heights based on opening type\n",
        "    if opening_type == \"door\":\n",
        "        # Doors start at the bottom of the wall\n",
        "        bottom_z = z_min\n",
        "        top_z = z_min + min(size[1], height * 0.8)  # Limit door height\n",
        "    else:  # window\n",
        "        # Windows are positioned between 1/3 and 2/3 of wall height\n",
        "        window_height = min(size[1], height * 0.4)  # Limit window height\n",
        "        bottom_z = z_min + height * 0.4 - window_height / 2\n",
        "        top_z = bottom_z + window_height\n",
        "\n",
        "    # X and Y position - use the wall's origin plus offset along direction\n",
        "    rel_x, rel_y = relative_position\n",
        "\n",
        "    # Adjust to keep within wall bounds\n",
        "    rel_x = max(0.05, min(0.95, rel_x))\n",
        "\n",
        "    # Calculate the horizontal offset along the wall\n",
        "    offset = width * rel_x\n",
        "\n",
        "    # Calculate the starting point\n",
        "    start_x = points[0][0] + direction[0] * offset\n",
        "    start_y = points[0][1] + direction[1] * offset\n",
        "\n",
        "    # Calculate perpendicular vector (pointing outward from wall)\n",
        "    perp = np.cross(direction, np.array([0, 0, 1]))\n",
        "    perp_length = np.linalg.norm(perp)\n",
        "    if perp_length > 0:\n",
        "        perp = perp / perp_length\n",
        "    else:\n",
        "        perp = np.array([0, 1, 0])  # Default fallback\n",
        "\n",
        "    # Calculate the four corners of the opening\n",
        "    half_width = size[0] / 2\n",
        "\n",
        "    # Bottom left, Bottom right, Top right, Top left, Bottom left (to close the ring)\n",
        "    corners = [\n",
        "        (start_x - direction[0] * half_width, start_y - direction[1] * half_width, bottom_z),\n",
        "        (start_x + direction[0] * half_width, start_y + direction[1] * half_width, bottom_z),\n",
        "        (start_x + direction[0] * half_width, start_y + direction[1] * half_width, top_z),\n",
        "        (start_x - direction[0] * half_width, start_y - direction[1] * half_width, top_z),\n",
        "        (start_x - direction[0] * half_width, start_y - direction[1] * half_width, bottom_z)\n",
        "    ]\n",
        "\n",
        "    # Flatten the coordinates\n",
        "    flat_coords = []\n",
        "    for corner in corners:\n",
        "        flat_coords.extend(corner)\n",
        "\n",
        "    return flat_coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXFYn3gxW8Cv"
      },
      "outputs": [],
      "source": [
        "def generate_lod25_from_predictions(input_gml_path, predictions, output_dir='lod2_5_output'):\n",
        "    \"\"\"\n",
        "    Generate LOD2.5 model by adding openings to existing LOD2 model\n",
        "\n",
        "    Parameters:\n",
        "    - input_gml_path: Path to input LOD2 GML file\n",
        "    - predictions: Dictionary mapping facade IDs to prediction arrays\n",
        "        {facade_id: {'prediction': pred_array, 'image': image_array}}\n",
        "    - output_dir: Directory to save output files\n",
        "\n",
        "    Returns:\n",
        "    - Path to output GML file\n",
        "    \"\"\"\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Parse the input GML file\n",
        "    parser = etree.XMLParser(remove_blank_text=True)\n",
        "    tree = etree.parse(input_gml_path, parser)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract all wall surfaces\n",
        "    wall_surfaces = extract_wall_surfaces(tree)\n",
        "    print(f\"Found {len(wall_surfaces)} wall surfaces in the GML file\")\n",
        "\n",
        "    # Count of openings added\n",
        "    window_count = 0\n",
        "    door_count = 0\n",
        "\n",
        "    # Process each wall that has a corresponding prediction\n",
        "    for wall in wall_surfaces:\n",
        "        wall_id = wall[\"id\"]\n",
        "        wall_element = wall[\"element\"]\n",
        "\n",
        "        if wall_id in predictions:\n",
        "            print(f\"Processing wall: {wall_id}\")\n",
        "            pred = predictions[wall_id]['prediction']\n",
        "\n",
        "            # Get wall dimensions from the first polygon's coordinates\n",
        "            if wall[\"polygons\"]:\n",
        "                wall_coords = wall[\"polygons\"][0][\"coords\"]\n",
        "                wall_width, wall_height = calculate_wall_dimensions(wall_coords)\n",
        "            else:\n",
        "                print(f\"Warning: No polygon coordinates found for wall {wall_id}\")\n",
        "                continue\n",
        "\n",
        "            # Find windows (class 1) in prediction\n",
        "            window_mask = (pred == 1).astype(np.uint8)\n",
        "            window_contours, _ = cv2.findContours(window_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            # Find doors (class 2) in prediction\n",
        "            door_mask = (pred == 2).astype(np.uint8)\n",
        "            door_contours, _ = cv2.findContours(door_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            pred_height, pred_width = pred.shape\n",
        "\n",
        "            # Process windows\n",
        "            for i, contour in enumerate(window_contours):\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "                area = cv2.contourArea(contour)\n",
        "\n",
        "                # Skip very small windows\n",
        "                if area < 100 or w < 10 or h < 10:\n",
        "                    continue\n",
        "\n",
        "                # Calculate relative position in prediction\n",
        "                rel_x = (x + w/2) / pred_width\n",
        "                rel_y = (y + h/2) / pred_height\n",
        "\n",
        "                # Scale window dimensions to real-world units\n",
        "                real_width = (w / pred_width) * wall_width * 0.7  # Scale down a bit\n",
        "                real_height = (h / pred_height) * wall_height * 0.7\n",
        "\n",
        "                # Generate opening coordinates\n",
        "                opening_coords = generate_opening_coords(\n",
        "                    wall_coords,\n",
        "                    (rel_x, rel_y),\n",
        "                    (real_width, real_height),\n",
        "                    \"window\"\n",
        "                )\n",
        "\n",
        "                if not opening_coords:\n",
        "                    print(f\"Warning: Failed to generate coordinates for window {i}\")\n",
        "                    continue\n",
        "\n",
        "                # Add window opening to the wall\n",
        "                opening_element = etree.SubElement(wall_element, f\"{{{ns_bldg}}}opening\")\n",
        "                window_element = etree.SubElement(opening_element, f\"{{{ns_bldg}}}Window\")\n",
        "                window_id = f\"Window_{wall_id.split('_')[-1]}_{i}\"\n",
        "                window_element.set(f\"{{{ns_gml}}}id\", window_id)\n",
        "\n",
        "                # Add LOD3 multi-surface\n",
        "                lod3_ms = etree.SubElement(window_element, f\"{{{ns_bldg}}}lod3MultiSurface\")\n",
        "                ms = etree.SubElement(lod3_ms, f\"{{{ns_gml}}}MultiSurface\")\n",
        "                sm = etree.SubElement(ms, f\"{{{ns_gml}}}surfaceMember\")\n",
        "                polygon = etree.SubElement(sm, f\"{{{ns_gml}}}Polygon\")\n",
        "                polygon.set(f\"{{{ns_gml}}}id\", f\"{window_id}_Polygon\")\n",
        "\n",
        "                # Add exterior\n",
        "                exterior = etree.SubElement(polygon, f\"{{{ns_gml}}}exterior\")\n",
        "                linear_ring = etree.SubElement(exterior, f\"{{{ns_gml}}}LinearRing\")\n",
        "                pos_list = etree.SubElement(linear_ring, f\"{{{ns_gml}}}posList\")\n",
        "                pos_list.set(\"srsDimension\", \"3\")\n",
        "                pos_list.text = \" \".join(map(str, opening_coords))\n",
        "\n",
        "                window_count += 1\n",
        "\n",
        "            # Process doors\n",
        "            for i, contour in enumerate(door_contours):\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "                area = cv2.contourArea(contour)\n",
        "\n",
        "                # Skip very small doors\n",
        "                if area < 200 or w < 15 or h < 30:\n",
        "                    continue\n",
        "\n",
        "                # Calculate relative position in prediction\n",
        "                rel_x = (x + w/2) / pred_width\n",
        "                rel_y = (y + h/2) / pred_height\n",
        "\n",
        "                # Scale door dimensions to real-world units\n",
        "                real_width = (w / pred_width) * wall_width * 0.7\n",
        "                real_height = (h / pred_height) * wall_height * 0.8\n",
        "\n",
        "                # Generate opening coordinates\n",
        "                opening_coords = generate_opening_coords(\n",
        "                    wall_coords,\n",
        "                    (rel_x, rel_y),\n",
        "                    (real_width, real_height),\n",
        "                    \"door\"\n",
        "                )\n",
        "\n",
        "                if not opening_coords:\n",
        "                    print(f\"Warning: Failed to generate coordinates for door {i}\")\n",
        "                    continue\n",
        "\n",
        "                # Add door opening to the wall\n",
        "                opening_element = etree.SubElement(wall_element, f\"{{{ns_bldg}}}opening\")\n",
        "                door_element = etree.SubElement(opening_element, f\"{{{ns_bldg}}}Door\")\n",
        "                door_id = f\"Door_{wall_id.split('_')[-1]}_{i}\"\n",
        "                door_element.set(f\"{{{ns_gml}}}id\", door_id)\n",
        "\n",
        "                # Add LOD3 multi-surface\n",
        "                lod3_ms = etree.SubElement(door_element, f\"{{{ns_bldg}}}lod3MultiSurface\")\n",
        "                ms = etree.SubElement(lod3_ms, f\"{{{ns_gml}}}MultiSurface\")\n",
        "                sm = etree.SubElement(ms, f\"{{{ns_gml}}}surfaceMember\")\n",
        "                polygon = etree.SubElement(sm, f\"{{{ns_gml}}}Polygon\")\n",
        "                polygon.set(f\"{{{ns_gml}}}id\", f\"{door_id}_Polygon\")\n",
        "\n",
        "                # Add exterior\n",
        "                exterior = etree.SubElement(polygon, f\"{{{ns_gml}}}exterior\")\n",
        "                linear_ring = etree.SubElement(exterior, f\"{{{ns_gml}}}LinearRing\")\n",
        "                pos_list = etree.SubElement(linear_ring, f\"{{{ns_gml}}}posList\")\n",
        "                pos_list.set(\"srsDimension\", \"3\")\n",
        "                pos_list.text = \" \".join(map(str, opening_coords))\n",
        "\n",
        "                door_count += 1\n",
        "\n",
        "    # Update LOD version in file\n",
        "    for city_model in root.findall(f\"{{{ns_citygml}}}CityModel\"):\n",
        "        name_element = city_model.find(f\"{{{ns_gml}}}name\")\n",
        "        if name_element is not None:\n",
        "            if \"LOD2\" in name_element.text:\n",
        "                name_element.text = name_element.text.replace(\"LOD2\", \"LOD2.5\")\n",
        "            else:\n",
        "                name_element.text = f\"LOD2.5_{datetime.now().strftime('%Y%m%d')}\"\n",
        "\n",
        "    # Save the modified GML file\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_filename = f\"LOD2.5_{Path(input_gml_path).stem}_{timestamp}.gml\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    # Write the XML file with pretty formatting\n",
        "    tree.write(output_path, pretty_print=True, xml_declaration=True, encoding=\"utf-8\")\n",
        "    print(f\"Added {window_count} windows and {door_count} doors to the model\")\n",
        "    print(f\"LOD2.5 model saved to: {output_path}\")\n",
        "\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "yT3KkisVW_yQ",
        "outputId": "9e3d3ee9-609c-4ea1-c0f4-a96a4dc5c3ac"
      },
      "outputs": [],
      "source": [
        "# Cell to run the LOD2.5 generation with your predictions\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Use the final prediction from the previous cells\n",
        "predictions = {\n",
        "    'DEBY_LOD2_4959457_e030de07-0827-4f7b-9657-9070e768b8f7': {'image': original_image, 'prediction': final_pred}\n",
        "}\n",
        "\n",
        "# Upload CityGML file\n",
        "print(\"Please upload your CityGML file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"No file was uploaded.\")\n",
        "else:\n",
        "    # Get the first uploaded file\n",
        "    filename = next(iter(uploaded.keys()))\n",
        "    input_gml_path = os.path.join(BASE_DIR, filename)\n",
        "\n",
        "    # Move the file to the right location\n",
        "    with open(input_gml_path, 'wb') as f:\n",
        "        f.write(uploaded[filename])\n",
        "\n",
        "    print(f\"File saved to: {input_gml_path}\")\n",
        "\n",
        "    # Generate LOD2.5 model\n",
        "    output_gml_path = generate_lod25_from_predictions(\n",
        "        input_gml_path,\n",
        "        predictions,\n",
        "        output_dir='lod2_5_output'\n",
        "    )\n",
        "\n",
        "    if output_gml_path and os.path.exists(output_gml_path):\n",
        "        print(f\"LOD2.5 model generated successfully at: {output_gml_path}\")\n",
        "        # Allow download of the generated file\n",
        "        files.download(output_gml_path)\n",
        "    else:\n",
        "        print(\"Failed to generate LOD2.5 model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "uKlWMyVuXB5l",
        "outputId": "6c9bfd6d-effc-480d-b020-a44e309a6843"
      },
      "outputs": [],
      "source": [
        "# Optional cell to visualize the generated LOD2.5 model (if you have a visualization library)\n",
        "\n",
        "def visualize_gml_results(prediction, output_gml_path):\n",
        "    \"\"\"\n",
        "    Create a comparison visualization between prediction and the resulting GML\n",
        "    This is just a placeholder - you would need to implement this with your preferred visualization method\n",
        "    \"\"\"\n",
        "    # Create a figure with two subplots\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot the prediction\n",
        "    plt.subplot(1, 2, 1)\n",
        "    class_colors = {\n",
        "        0: [105, 105, 105],  # Facade - Gray\n",
        "        1: [255, 255, 0],    # Window - Yellow\n",
        "        2: [139, 69, 19],    # Door - Brown\n",
        "        3: [220, 220, 220]   # Unknown - Light Gray\n",
        "    }\n",
        "    pred_visualization = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)\n",
        "    for class_idx, color in class_colors.items():\n",
        "        pred_visualization[prediction == class_idx] = color\n",
        "\n",
        "    plt.imshow(pred_visualization)\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot the generated GML (placeholder - actual visualization depends on your tools)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.text(0.5, 0.5, f\"GML generated at:\\n{output_gml_path}\",\n",
        "             horizontalalignment='center', verticalalignment='center',\n",
        "             transform=plt.gca().transAxes)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Generated LOD2.5 GML\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment to use this visualization after generating the model\n",
        "if output_gml_path and os.path.exists(output_gml_path):\n",
        "  visualize_gml_results(final_pred, output_gml_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
